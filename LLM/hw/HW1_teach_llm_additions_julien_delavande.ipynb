{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"80517dbc","cell_type":"markdown","source":"# Teach an LLM to do additions","metadata":{"id":"80517dbc"}},{"id":"0aaca18f","cell_type":"markdown","source":"The goal of this project is to teach an LLM to do additions, playing only with two parts:\n* the tokenizer\n* the positional embedding\n\nBoth the model and the dataset are fixed.\n\nYou are allowed to tune the hyperparameters, but this is not the main goal. Depending on the quality of your tokenizer and positional embedding, you may change the number of bits. The initial value of 3 is very small.","metadata":{}},{"id":"ae993bb9","cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nimport random\nimport math\nimport re\nimport time","metadata":{"id":"ae993bb9","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:45:30.320651Z","iopub.execute_input":"2025-02-10T10:45:30.320998Z","iopub.status.idle":"2025-02-10T10:45:30.324969Z","shell.execute_reply.started":"2025-02-10T10:45:30.320972Z","shell.execute_reply":"2025-02-10T10:45:30.324139Z"}},"outputs":[],"execution_count":27},{"id":"OzGh9ahKF17h","cell_type":"code","source":"number_bits = 9\n\ndataset_size = 6400_000\ntrain_proportion = 0.9\n\nlog_interval = 200\nbatch_size = 256\nepochs = 1\nlearning_rate = 8e-4","metadata":{"id":"OzGh9ahKF17h","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:45:30.325984Z","iopub.execute_input":"2025-02-10T10:45:30.326234Z","iopub.status.idle":"2025-02-10T10:45:30.339174Z","shell.execute_reply.started":"2025-02-10T10:45:30.326214Z","shell.execute_reply":"2025-02-10T10:45:30.338415Z"}},"outputs":[],"execution_count":28},{"id":"6c054bed","cell_type":"markdown","source":"## Step 1: Construct a tokenizer","metadata":{"id":"6c054bed"}},{"id":"t6aC9uNeIR6C","cell_type":"code","source":"pad_token=\"[PAD]\"\neos_token=\"[EOS]\"","metadata":{"id":"t6aC9uNeIR6C","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:45:30.340480Z","iopub.execute_input":"2025-02-10T10:45:30.340772Z","iopub.status.idle":"2025-02-10T10:45:30.351371Z","shell.execute_reply.started":"2025-02-10T10:45:30.340747Z","shell.execute_reply":"2025-02-10T10:45:30.350631Z"}},"outputs":[],"execution_count":29},{"id":"BMvT0B-MGBnY","cell_type":"markdown","source":"### Baseline: character-level tokenizer","metadata":{"id":"BMvT0B-MGBnY"}},{"id":"g2QiF-otFur3","cell_type":"code","source":"class character_level_tokenizer:\n    \"\"\"\n    character-level\n    \"\"\"\n    def __init__(self):\n        pad_token=\"[PAD]\"\n        eos_token=\"[EOS]\"\n        self.vocab = [str(x) for x in range(10)] + [\"+\", \"=\"] + [pad_token, eos_token]\n        self.token_to_id = {v : k for k, v in enumerate(self.vocab)}\n        self.id_to_token = {k : v for k, v in enumerate(self.vocab)}\n        self.ntokens = len(self.vocab)\n        self.pattern = f\"[^{re.escape(''.join(self.vocab))}]\"\n    \n    def clean(self, text):\n        \"\"\"\n        removes all characters not in the vocabulary\n        \"\"\"\n        out = re.sub(self.pattern, \"\", text)\n        return out\n\n    def pre_tokenization(self, text):\n        \"\"\"\n        character-level\n        \"\"\"\n        return [c for c in text]\n\n    def encode(self, text):\n        text_list = self.pre_tokenization(self.clean(text))\n        return [self.token_to_id[c] for c in text_list]\n\n    def decode(self, token_list):\n        return \"\".join([self.id_to_token[x] for x in token_list])\n\n","metadata":{"id":"g2QiF-otFur3","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:45:30.352555Z","iopub.execute_input":"2025-02-10T10:45:30.352838Z","iopub.status.idle":"2025-02-10T10:45:30.364438Z","shell.execute_reply.started":"2025-02-10T10:45:30.352810Z","shell.execute_reply":"2025-02-10T10:45:30.363778Z"}},"outputs":[],"execution_count":30},{"id":"QuCc6jF5F8hK","cell_type":"code","source":"tokenizer = character_level_tokenizer()\nntokens = tokenizer.ntokens\nntokens","metadata":{"id":"QuCc6jF5F8hK","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:45:30.365211Z","iopub.execute_input":"2025-02-10T10:45:30.365480Z","iopub.status.idle":"2025-02-10T10:45:30.381754Z","shell.execute_reply.started":"2025-02-10T10:45:30.365452Z","shell.execute_reply":"2025-02-10T10:45:30.380933Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"14"},"metadata":{}}],"execution_count":31},{"id":"8FXW2K-1Jd-P","cell_type":"code","source":"prompt = \"129564 + 42456 =\"\ninputs = tokenizer.encode(prompt)\ninputs, tokenizer.decode(inputs)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8FXW2K-1Jd-P","outputId":"349a4033-9fce-462b-f0d5-1bb3a7ffd340","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:45:30.382595Z","iopub.execute_input":"2025-02-10T10:45:30.382829Z","iopub.status.idle":"2025-02-10T10:45:30.394634Z","shell.execute_reply.started":"2025-02-10T10:45:30.382799Z","shell.execute_reply":"2025-02-10T10:45:30.393903Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"([1, 2, 9, 5, 6, 4, 10, 4, 2, 4, 5, 6, 11], '129564+42456=')"},"metadata":{}}],"execution_count":32},{"id":"j3gckvebGGYt","cell_type":"markdown","source":"# Implement your tokenizer here!\n\nYou can do anything (as long as you do not compute the addition!).\nSome ideas:\n* reversing numbers left to right\n* arranging by groups (of, 2, 3,...)\n* aligning numbers","metadata":{"id":"j3gckvebGGYt"}},{"id":"a0002b32-6d7b-49eb-959e-b325b437e0d6","cell_type":"code","source":"\nclass ColumnWiseAdditionTokenizer:\n    def __init__(self):\n        self.vocab = [str(i) for i in range(11)] + [\"+\", \"=\", pad_token, eos_token]\n        self.token_to_id = {v: k for k, v in enumerate(self.vocab)}\n        self.id_to_token = {k: v for v, k in self.token_to_id.items()}\n        self.ntokens = len(self.vocab)\n        self.pattern = f\"[^{re.escape(''.join(self.vocab))}]\"\n\n    def clean(self, text):\n        \"\"\"\n        Removes all characters not in the vocabulary\n        \"\"\"\n        return re.sub(self.pattern, \"\", text)\n\n    def pre_tokenization(self, num1, num2=None):\n        \"\"\"\n        Returns a list of characters.\n        Tokenizes an addition into a column-wise addition sequence\n        If only num1 is provided, it returns a simple tokenized number.\n        \"\"\"\n        num1 = str(num1)\n        if num2 is None:\n            return list(num1)\n        \n        num2 = str(num2)\n        max_len = max(len(num1), len(num2))\n        num1, num2 = num1.zfill(max_len), num2.zfill(max_len)  # Padding with zeros\n        \n        tokens = []\n        for i in range(max_len - 1, -1, -1):  # Start from least significant digit\n            tokens.append(num1[i])\n            tokens.append(\"+\")\n            tokens.append(num2[i])\n            tokens.append(\"=\")\n        return tokens\n    \n    def encode(self, text):\n        \"\"\"\n        Extract num1 and num2 from the string input and tokenize accordingly\n        \"\"\"\n        match = re.match(r\"(\\d+)\\+(\\d+)=\", text)\n        if match:\n            num1, num2 = match.groups()\n            token_list = self.pre_tokenization(num1, num2)\n        else:\n            # If the input is just a number, encode it as digits\n            token_list = self.pre_tokenization(text)\n        \n        return [self.token_to_id[t] for t in token_list if t in self.token_to_id]\n    \n    def decode(self, token_list):\n        return \"\".join([self.id_to_token[x] for x in token_list if x in self.id_to_token])\n\n# Exemple d'utilisation\ntokenizer = ColumnWiseAdditionTokenizer()\nntokens = tokenizer.ntokens\nencoded = tokenizer.encode('703+499=')\nprint(\"Encoded:\", encoded)\nprint(\"Decoded:\", tokenizer.decode(encoded))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:45:30.396263Z","iopub.execute_input":"2025-02-10T10:45:30.396461Z","iopub.status.idle":"2025-02-10T10:45:30.409513Z","shell.execute_reply.started":"2025-02-10T10:45:30.396443Z","shell.execute_reply":"2025-02-10T10:45:30.408707Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Encoded: [3, 11, 9, 12, 0, 11, 9, 12, 7, 11, 4, 12]\nDecoded: 3+9=0+9=7+4=\n","output_type":"stream"}],"execution_count":33},{"id":"82847227-f842-4a5f-a7b9-369a5d5d9b28","cell_type":"markdown","source":"I designed a **character-level tokenizer** for column-wise addition by structuring input in a way that aligns digits like manual addition. It removes unsupported characters, then **formats numbers column-wise** by reversing operands (if `reverse=True`) and padding with zeros where needed. This ensures that each digit pair aligns properly, making it easier for models to learn digit-wise dependencies. The tokenizer maps characters to token IDs for encoding and reconstructs expressions from IDs for decoding, preserving the structured representation of addition problems.","metadata":{}},{"id":"45c1272e-f1e1-4478-82a1-ebf747d0c5ed","cell_type":"code","source":"class ColumnWiseAdditionTokenizer:\n    \"\"\"\n    character-level\n    \"\"\"\n    def __init__(self, reverse=True):\n        self.reverse = reverse\n        pad_token=\"[PAD]\"\n        eos_token=\"[EOS]\"\n        # self.cols = [\"a\", \"b\", \"c\"]\n        self.vocab = [str(x) for x in range(10)] + [\"+\", \"=\"] + [pad_token, eos_token]\n        # self.vocab = [str(x) for x in range(10)] + [\"+\", \"=\", '#'] + [pad_token, eos_token]\n        self.token_to_id = {v : k for k, v in enumerate(self.vocab)}\n        self.id_to_token = {k : v for k, v in enumerate(self.vocab)}\n        self.ntokens = len(self.vocab)\n        self.pattern = f\"[^{re.escape(''.join(self.vocab))}]\"\n\n    def clean(self, text):\n        \"\"\"\n        removes all characters not in the vocabulary\n        \"\"\"\n        out = re.sub(self.pattern, \"\", text)\n        return out\n\n    def pre_tokenization(self, text):\n        \"\"\"\n        character-level\n        \"\"\"\n\n        if self.reverse:\n          if '+' in text:\n            a, b = text.split('+')\n            c = ''\n            if '=' in text:\n              b, c = b.split('=')\n            #a, b, c = a[::-1], b[::-1], c[::-1]\n            a, b, c = a[::-1], b[::-1], c\n            ret = ''\n            for i in range(number_bits):\n                #ret += '+'\n                if i < len(a):\n                   ret += a[i]\n                else: ret += '0'\n                ret += '+'\n                if i < len(b):\n                   ret += b[i]\n                else: ret += '0' \n                ret += '='\n            text = ret[:-1] + '=' + c\n          else:\n            text = text\n        return [c for c in text]\n\n    def encode(self, text):\n        text_list = self.pre_tokenization(self.clean(text))\n        return [self.token_to_id[c] for c in text_list]\n\n    def decode(self, token_list):\n        return \"\".join([self.id_to_token[x] for x in token_list if x in self.id_to_token])\n\n# Exemple d'utilisation\ntokenizer = ColumnWiseAdditionTokenizer()\nntokens = tokenizer.ntokens\nencoded = tokenizer.encode('3+99=')\nprint(\"Encoded:\", encoded)\nprint(\"Decoded:\", tokenizer.decode(encoded))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:45:30.526540Z","iopub.execute_input":"2025-02-10T10:45:30.526829Z","iopub.status.idle":"2025-02-10T10:45:30.536437Z","shell.execute_reply.started":"2025-02-10T10:45:30.526798Z","shell.execute_reply":"2025-02-10T10:45:30.535737Z"}},"outputs":[{"name":"stdout","text":"Encoded: [3, 10, 9, 11, 0, 10, 9, 11, 0, 10, 0, 11, 0, 10, 0, 11, 0, 10, 0, 11, 0, 10, 0, 11, 0, 10, 0, 11, 0, 10, 0, 11, 0, 10, 0, 11]\nDecoded: 3+9=0+9=0+0=0+0=0+0=0+0=0+0=0+0=0+0=\n","output_type":"stream"}],"execution_count":34},{"id":"491af297","cell_type":"markdown","source":"## Step 2: Create a dataset for arithmetic operations","metadata":{"id":"491af297"}},{"id":"daa90f31","cell_type":"code","source":"def sample_datapoint(number_bits = 9):\n    \"\"\"\n    returns a string containing two random numbers on `number_bits` many bits and their sum.\n    \"\"\"\n    a_list = [random.randint(0, 9) for _ in range(number_bits)]\n    b_list = [random.randint(0, 9) for _ in range(number_bits)]\n    a_int = int(\"\".join([str(x) for x in a_list]))\n    b_int = int(\"\".join([str(x) for x in b_list]))\n    sum_int = a_int + b_int\n    return (str(a_int) + \"+\" + str(b_int) + \"=\", str(sum_int))\n\nsample_datapoint(3)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"daa90f31","outputId":"3e8719ee-d8fa-4984-8b51-4db3457f7dbc","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:45:30.537505Z","iopub.execute_input":"2025-02-10T10:45:30.537716Z","iopub.status.idle":"2025-02-10T10:45:30.553918Z","shell.execute_reply.started":"2025-02-10T10:45:30.537699Z","shell.execute_reply":"2025-02-10T10:45:30.553280Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"('956+68=', '1024')"},"metadata":{}}],"execution_count":35},{"id":"b6e861d2","cell_type":"code","source":"data = []\nfor _ in range(dataset_size):\n    data.append(sample_datapoint(number_bits))\ndata[:4]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b6e861d2","outputId":"c88c2226-0546-473c-c296-88a52823886b","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:45:30.555118Z","iopub.execute_input":"2025-02-10T10:45:30.555338Z","iopub.status.idle":"2025-02-10T10:47:24.051146Z","shell.execute_reply.started":"2025-02-10T10:45:30.555319Z","shell.execute_reply":"2025-02-10T10:47:24.050433Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"[('763440039+143692342=', '907132381'),\n ('68201147+490557048=', '558758195'),\n ('468870653+397800144=', '866670797'),\n ('290244901+712442342=', '1002687243')]"},"metadata":{}}],"execution_count":36},{"id":"fee85050","cell_type":"code","source":"data_train = data[: int(train_proportion * dataset_size)]\ndata_test = data[int(train_proportion * dataset_size):]\n\nlen(data_train),len(data_test)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fee85050","outputId":"f080f4b0-fd76-48d8-d59f-7c118b6e6fe9","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:47:24.052101Z","iopub.execute_input":"2025-02-10T10:47:24.052422Z","iopub.status.idle":"2025-02-10T10:47:24.594524Z","shell.execute_reply.started":"2025-02-10T10:47:24.052398Z","shell.execute_reply":"2025-02-10T10:47:24.593766Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"(5760000, 640000)"},"metadata":{}}],"execution_count":37},{"id":"37200598","cell_type":"markdown","source":"## Step 3: Construct a model","metadata":{"id":"37200598"}},{"id":"0fd7d2eb","cell_type":"markdown","source":"### Basline: the classical Positional Embedding","metadata":{}},{"id":"91674239","cell_type":"code","source":"class PositionalEmbedding(nn.Module):\n    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n        Here, we use sine and cosine functions of different frequencies.\n    .. math:\n        \\text{PosEmbedder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n        \\text{PosEmbedder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n        \\text{where pos is the word position and i is the embed idx)\n    Args:\n        d_model: the embed dim (required).\n        dropout: the dropout value (default=0.1).\n        max_len: the max. length of the incoming sequence (default=5000).\n    \"\"\"\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEmbedding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        r\"\"\"Inputs of forward function\n        Args:\n            x: the sequence fed to the positional encoder model (required).\n        Shape:\n            x: [sequence length, batch size, embed dim]\n            output: [sequence length, batch size, embed dim]\n        \"\"\"\n\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)","metadata":{"id":"91674239","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:47:24.595423Z","iopub.execute_input":"2025-02-10T10:47:24.595744Z","iopub.status.idle":"2025-02-10T10:47:24.608547Z","shell.execute_reply.started":"2025-02-10T10:47:24.595712Z","shell.execute_reply":"2025-02-10T10:47:24.607891Z"}},"outputs":[],"execution_count":38},{"id":"8296ceb2","cell_type":"markdown","source":"# Implement your positional embedding here!\n\nYou can do anything. Some ideas:\n* RoPE\n* (randomised) FIRE\n* Abacus\n\n**!!! IMPORTANT !!!** This model of Transformers is \"input first\", meaning that an input is a tensor with shape\n(length_prompts, batch_size)","metadata":{}},{"id":"0605807b-ec63-42ee-b716-63ab068c67d6","cell_type":"markdown","source":"I implemented a **learnable positional embedding** module that replaces traditional sinusoidal encodings with trainable position-dependent vectors. Each position in the sequence is assigned a **learnable embedding**, initialized with a normal distribution, allowing the model to **adapt positional information dynamically**. The embeddings are added to the input token embeddings, ensuring the model retains order information while learning optimal representations.","metadata":{}},{"id":"cd8dd506-d119-4dc9-9978-033d388fcb49","cell_type":"code","source":"class PositionalEmbedding(nn.Module):\n    r\"\"\"Inject learnable positional embeddings instead of using fixed sinusoidal functions.\n    \n    Args:\n        d_model: the embedding dimension (required).\n        dropout: the dropout value (default=0.1).\n        max_len: the max. length of the incoming sequence (default=5000).\n    \"\"\"\n    \n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEmbedding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        # Learnable positional embeddings\n        self.pos_embedding = nn.Parameter(torch.zeros(max_len, 1, d_model))\n        nn.init.normal_(self.pos_embedding, mean=0, std=0.02)  # Initialize embeddings\n\n    def forward(self, x):\n        r\"\"\"Inputs of forward function\n        Args:\n            x: the sequence fed to the positional encoder model (required).\n        Shape:\n            x: [sequence length, batch size, embed dim]\n            output: [sequence length, batch size, embed dim]\n        \"\"\"\n        x = x + self.pos_embedding[:x.size(0), :]\n        return self.dropout(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:47:24.610329Z","iopub.execute_input":"2025-02-10T10:47:24.610525Z","iopub.status.idle":"2025-02-10T10:47:24.625942Z","shell.execute_reply.started":"2025-02-10T10:47:24.610498Z","shell.execute_reply":"2025-02-10T10:47:24.625235Z"}},"outputs":[],"execution_count":39},{"id":"4eb278ab","cell_type":"code","source":"class TransformerModel(nn.Transformer):\n    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n        super(TransformerModel, self).__init__(d_model=ninp,\n                                               nhead=nhead,\n                                               dim_feedforward=nhid,\n                                               num_encoder_layers=nlayers)\n        self.input_emb = nn.Embedding(ntoken, ninp)\n        self.pos_encoder = PositionalEmbedding(ninp, dropout)\n        self.decoder = nn.Linear(ninp, ntoken)\n\n        self.ninp = ninp\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n        nn.init.zeros_(self.decoder.bias)\n        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n\n    def _generate_square_subsequent_mask(self, sz):\n        return torch.log(torch.tril(torch.ones(sz,sz)))\n\n    def forward(self, src):\n        mask = self._generate_square_subsequent_mask(len(src)).to(device)\n        self.src_mask = mask\n\n        src = self.input_emb(src) * math.sqrt(self.ninp)\n        src = self.pos_encoder(src)\n        output_enc = self.encoder(src, mask=self.src_mask)\n        output_dec = self.decoder(output_enc)\n        return F.log_softmax(output_dec, dim=-1), output_enc","metadata":{"id":"4eb278ab","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:47:24.627227Z","iopub.execute_input":"2025-02-10T10:47:24.627461Z","iopub.status.idle":"2025-02-10T10:47:24.643073Z","shell.execute_reply.started":"2025-02-10T10:47:24.627442Z","shell.execute_reply":"2025-02-10T10:47:24.642307Z"}},"outputs":[],"execution_count":40},{"id":"42f9d1ee","cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:47:24.643860Z","iopub.execute_input":"2025-02-10T10:47:24.644118Z","iopub.status.idle":"2025-02-10T10:47:24.658952Z","shell.execute_reply.started":"2025-02-10T10:47:24.644098Z","shell.execute_reply":"2025-02-10T10:47:24.658187Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":41},{"id":"a30e093a","cell_type":"markdown","source":"Please do not change these parameters!","metadata":{}},{"id":"1d568cc4","cell_type":"code","source":"model = TransformerModel(ntoken = ntokens,\n                         ninp = 128,\n                         nhead = 16,\n                         nhid = 64,\n                         nlayers = 8)\nmodel.to(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1d568cc4","outputId":"f7f78975-2bdf-4c36-de35-3e140636d476","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:47:24.659793Z","iopub.execute_input":"2025-02-10T10:47:24.660012Z","iopub.status.idle":"2025-02-10T10:47:25.017175Z","shell.execute_reply.started":"2025-02-10T10:47:24.659994Z","shell.execute_reply":"2025-02-10T10:47:25.016340Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(\n","output_type":"stream"},{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"TransformerModel(\n  (encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0-7): 8 x TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n        )\n        (linear1): Linear(in_features=128, out_features=64, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=64, out_features=128, bias=True)\n        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n  )\n  (decoder): Linear(in_features=128, out_features=14, bias=True)\n  (input_emb): Embedding(14, 128)\n  (pos_encoder): PositionalEmbedding(\n    (dropout): Dropout(p=0.5, inplace=False)\n  )\n)"},"metadata":{}}],"execution_count":42},{"id":"8f2f06e0","cell_type":"code","source":"def generate(model, prompts, new_tokens = 11):\n    input_tensor = prompts # (length_prompts, batch_size)\n    input_tensor = input_tensor.to(device)\n    for _ in range(new_tokens):\n        output, _ = model(input_tensor) # (length_prompts, batch_size, ntokens)\n        last_output = output[-1,:,:] # (batch_size, ntokens)\n        token = torch.argmax(last_output, -1).view((1,-1)) # (1, batch_size)\n        input_tensor = torch.cat((input_tensor, token), 0)\n    return input_tensor","metadata":{"id":"8f2f06e0","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:47:25.018133Z","iopub.execute_input":"2025-02-10T10:47:25.018492Z","iopub.status.idle":"2025-02-10T10:47:25.022833Z","shell.execute_reply.started":"2025-02-10T10:47:25.018453Z","shell.execute_reply":"2025-02-10T10:47:25.022077Z"}},"outputs":[],"execution_count":43},{"id":"d76d1b19","cell_type":"code","source":"model.eval()\n\nprompt = \"2+3=\"\nprompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\noutput = generate(model, prompt_tensor).view((1,-1))\noutput, tokenizer.decode(output.tolist()[0])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d76d1b19","outputId":"a1df1dc9-2ecc-4de4-85b2-6bc5bd460439","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:47:25.023568Z","iopub.execute_input":"2025-02-10T10:47:25.023824Z","iopub.status.idle":"2025-02-10T10:47:25.101540Z","shell.execute_reply.started":"2025-02-10T10:47:25.023803Z","shell.execute_reply":"2025-02-10T10:47:25.100913Z"}},"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"(tensor([[ 2, 10,  3, 11,  0, 10,  0, 11,  0, 10,  0, 11,  0, 10,  0, 11,  0, 10,\n           0, 11,  0, 10,  0, 11,  0, 10,  0, 11,  0, 10,  0, 11,  0, 10,  0, 11,\n           7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7]], device='cuda:0'),\n '2+3=0+0=0+0=0+0=0+0=0+0=0+0=0+0=0+0=77777777777')"},"metadata":{}}],"execution_count":44},{"id":"00954ddc","cell_type":"code","source":"def pad(token_list, type_list = \"prompts\"):\n    max_length = max([len(x) for x in token_list])\n    out = []\n    for x in token_list:\n        if type_list == \"prompts\":\n            out.append([tokenizer.token_to_id[pad_token]] * (max_length - len(x)) + x)\n        if type_list == \"answers\":\n            out.append(x + [tokenizer.token_to_id[eos_token]] + [tokenizer.token_to_id[pad_token]] * (max_length - len(x)))\n    return out, max_length","metadata":{"id":"00954ddc","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:47:25.102372Z","iopub.execute_input":"2025-02-10T10:47:25.102684Z","iopub.status.idle":"2025-02-10T10:47:25.107125Z","shell.execute_reply.started":"2025-02-10T10:47:25.102638Z","shell.execute_reply":"2025-02-10T10:47:25.106188Z"}},"outputs":[],"execution_count":45},{"id":"2c84beab","cell_type":"code","source":"prompts = [tokenizer.encode(\"1+1=\"), tokenizer.encode(\"21+35=\")]\nanswers = [tokenizer.encode(\"2\"), tokenizer.encode(\"56\")]\npadded_prompts, _ = pad(prompts, \"prompts\")\npadded_answers, _ = pad(answers, \"answers\")\npadded_prompts, padded_answers\n[tokenizer.decode(p) for p in padded_prompts], [tokenizer.decode(p) for p in padded_answers]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2c84beab","outputId":"fc1bea13-d6e1-4a55-b70d-36de00bcec9b","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:47:25.108012Z","iopub.execute_input":"2025-02-10T10:47:25.108300Z","iopub.status.idle":"2025-02-10T10:47:25.122599Z","shell.execute_reply.started":"2025-02-10T10:47:25.108269Z","shell.execute_reply":"2025-02-10T10:47:25.121953Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"(['1+1=0+0=0+0=0+0=0+0=0+0=0+0=0+0=0+0=',\n  '1+5=2+3=0+0=0+0=0+0=0+0=0+0=0+0=0+0='],\n ['2[EOS][PAD]', '56[EOS]'])"},"metadata":{}}],"execution_count":46},{"id":"264f9227","cell_type":"code","source":"def get_batch(split, i):\n    data = data_train if split == 'train' else data_test\n    prompts = [tokenizer.encode(data[i][0]) for i in range(i, i + batch_size)]\n    padded_prompts, length_prompts = pad(prompts, \"prompts\")\n    answers = [tokenizer.encode(data[i][1]) for i in range(i, i + batch_size)]\n    padded_answers, length_answers = pad(answers, \"answers\")\n    X = torch.stack([torch.tensor(x) for x in padded_prompts], 1)\n    Y = torch.stack([torch.tensor(x) for x in padded_answers], 1)\n    return X, Y, length_prompts, length_answers","metadata":{"id":"264f9227","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:47:25.124967Z","iopub.execute_input":"2025-02-10T10:47:25.125203Z","iopub.status.idle":"2025-02-10T10:47:25.136251Z","shell.execute_reply.started":"2025-02-10T10:47:25.125172Z","shell.execute_reply":"2025-02-10T10:47:25.135508Z"}},"outputs":[],"execution_count":47},{"id":"91e281ad","cell_type":"code","source":"X, Y, length_prompts, length_answers = get_batch(\"train\", 243)\nX.shape, Y.shape, length_prompts, length_answers","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"91e281ad","outputId":"22e2d0ee-ede4-41f8-e089-fb63ac2d9787","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:47:25.137147Z","iopub.execute_input":"2025-02-10T10:47:25.137361Z","iopub.status.idle":"2025-02-10T10:47:25.160816Z","shell.execute_reply.started":"2025-02-10T10:47:25.137342Z","shell.execute_reply":"2025-02-10T10:47:25.160127Z"}},"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"(torch.Size([36, 256]), torch.Size([11, 256]), 36, 10)"},"metadata":{}}],"execution_count":48},{"id":"113e1fd1","cell_type":"markdown","source":"## Step 4: Evaluate","metadata":{"id":"113e1fd1"}},{"id":"1cfcd10a","cell_type":"code","source":"def evaluate():\n    # Turn on evaluation mode disables dropout.\n    model.eval()\n    correct = 0.\n    with torch.no_grad():\n        for batch, i in enumerate(range(0, len(data_test) - 1, batch_size)):\n            prompts, target_answers, length_prompts, length_answers = get_batch(\"test\", i)\n            prompts = prompts.to(device) # (length_prompts, batch_size)\n            target_answers = target_answers.to(device) # (length_answers + 1, batch_size)\n            output = generate(model, prompts, length_answers + 1) # (length_prompts + length_answers + 1, batch_size)\n            answers_tokens = output[length_prompts:, :] # (length_answers + 1, batch_size), contains tokens\n            equality_test = answers_tokens == target_answers # (length_answers + 1, batch_size), contains boolean values\n            correct += torch.all(equality_test, axis=0).float().sum()\n        accuracy = correct / len(data_test)\n    return accuracy.item()","metadata":{"id":"1cfcd10a","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:47:25.161445Z","iopub.execute_input":"2025-02-10T10:47:25.161622Z","iopub.status.idle":"2025-02-10T10:47:25.172156Z","shell.execute_reply.started":"2025-02-10T10:47:25.161606Z","shell.execute_reply":"2025-02-10T10:47:25.171376Z"}},"outputs":[],"execution_count":49},{"id":"ac335b05","cell_type":"code","source":"evaluate()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ac335b05","outputId":"b475e943-51b3-401d-d18b-c9d32a49ffb6","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T13:35:24.415271Z","iopub.execute_input":"2025-02-10T13:35:24.415607Z","iopub.status.idle":"2025-02-10T13:43:58.312249Z","shell.execute_reply.started":"2025-02-10T13:35:24.415582Z","shell.execute_reply":"2025-02-10T13:43:58.311393Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"0.9998890161514282"},"metadata":{}}],"execution_count":5},{"id":"4c54061a","cell_type":"markdown","source":"## Step 4: Train the model","metadata":{"id":"4c54061a"}},{"id":"3638a75d","cell_type":"code","source":"def train_epoch():\n    model.train()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n    total_loss = 0.\n    start_time = time.time()\n    for batch, i in enumerate(range(0, len(data_train) - 1, batch_size)):\n        prompts, target_answers, length_prompts, length_answers = get_batch(\"train\", i)\n        prompts = prompts.to(device) # (length_prompts, batch_size)\n        target_answers = target_answers.to(device) # (length_answers, batch_size)\n        input_tensor = torch.cat((prompts, target_answers), 0) # (length_prompts + length_answers, batch_size)\n        model.zero_grad()\n        output, _ = model(input_tensor) # (length_prompts + length_answers, batch_size, ntokens)\n        output_answers = output[length_prompts-1:-1,:,:].reshape(-1, ntokens) # (length_answers * batch_size, ntokens)\n        target_answers = target_answers.view(-1)\n        loss = F.cross_entropy(output_answers, target_answers)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n        if batch % log_interval == 0 and batch > 0:\n            cur_loss = total_loss / log_interval\n            elapsed = time.time() - start_time\n            print('| {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.2f} | perplexity {:8.2f}'.format(batch, len(data_train) // batch_size,\n                                                                                                        elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n            total_loss = 0\n            start_time = time.time()\n\ndef train():\n    best_test_accuracy = None\n    test_accuracy = evaluate()\n    print('-' * 89)\n    print('| initialisation | test accuracy {:5.2f}'.format(test_accuracy))\n    print('-' * 89)\n    for epoch in range(1, epochs+1):\n        epoch_start_time = time.time()\n        train_epoch()\n        test_accuracy = evaluate()\n        print('-' * 89)\n        print('| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:5.2f}'.format(epoch, (time.time() - epoch_start_time), test_accuracy))\n        print('-' * 89)\n        # Save the model if the test accuracy is the best we've seen so far.\n        if not best_test_accuracy or test_accuracy < best_test_accuracy:\n            with open(\"arithmetic.pt\", 'wb') as f:\n                torch.save(model, f)\n            best_test_accuracy = test_accuracy","metadata":{"id":"3638a75d","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:55:43.398353Z","iopub.execute_input":"2025-02-10T10:55:43.398649Z","iopub.status.idle":"2025-02-10T10:55:43.406476Z","shell.execute_reply.started":"2025-02-10T10:55:43.398627Z","shell.execute_reply":"2025-02-10T10:55:43.405639Z"}},"outputs":[],"execution_count":51},{"id":"4e2a8490","cell_type":"code","source":"train()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4e2a8490","outputId":"f70dcac2-5891-4266-8748-85df050f4881","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:55:43.407184Z","iopub.execute_input":"2025-02-10T10:55:43.407396Z"}},"outputs":[{"name":"stdout","text":"-----------------------------------------------------------------------------------------\n| initialisation | test accuracy  0.00\n-----------------------------------------------------------------------------------------\n|   200/22500 batches | ms/batch 86.26 | loss  2.20 | perplexity     8.98\n|   400/22500 batches | ms/batch 87.93 | loss  1.97 | perplexity     7.18\n|   600/22500 batches | ms/batch 85.82 | loss  1.89 | perplexity     6.65\n|   800/22500 batches | ms/batch 85.62 | loss  1.79 | perplexity     6.01\n|  1000/22500 batches | ms/batch 85.65 | loss  1.75 | perplexity     5.74\n|  1200/22500 batches | ms/batch 85.63 | loss  1.69 | perplexity     5.44\n|  1400/22500 batches | ms/batch 85.49 | loss  1.31 | perplexity     3.72\n|  1600/22500 batches | ms/batch 87.55 | loss  0.77 | perplexity     2.16\n|  1800/22500 batches | ms/batch 85.52 | loss  0.48 | perplexity     1.61\n|  2000/22500 batches | ms/batch 85.62 | loss  0.33 | perplexity     1.38\n|  2200/22500 batches | ms/batch 85.76 | loss  0.34 | perplexity     1.40\n|  2400/22500 batches | ms/batch 85.83 | loss  0.18 | perplexity     1.20\n|  2600/22500 batches | ms/batch 86.13 | loss  0.14 | perplexity     1.15\n|  2800/22500 batches | ms/batch 85.82 | loss  0.12 | perplexity     1.12\n|  3000/22500 batches | ms/batch 88.05 | loss  0.10 | perplexity     1.10\n|  3200/22500 batches | ms/batch 85.81 | loss  0.08 | perplexity     1.09\n|  3400/22500 batches | ms/batch 85.78 | loss  0.08 | perplexity     1.08\n|  3600/22500 batches | ms/batch 85.86 | loss  0.07 | perplexity     1.07\n|  3800/22500 batches | ms/batch 85.65 | loss  0.06 | perplexity     1.06\n|  4000/22500 batches | ms/batch 85.76 | loss  0.19 | perplexity     1.21\n|  4200/22500 batches | ms/batch 87.80 | loss  0.05 | perplexity     1.06\n|  4400/22500 batches | ms/batch 85.73 | loss  0.04 | perplexity     1.04\n|  4600/22500 batches | ms/batch 85.75 | loss  0.05 | perplexity     1.05\n|  4800/22500 batches | ms/batch 85.65 | loss  0.04 | perplexity     1.04\n|  5000/22500 batches | ms/batch 85.70 | loss  0.03 | perplexity     1.03\n|  5200/22500 batches | ms/batch 85.71 | loss  0.04 | perplexity     1.04\n|  5400/22500 batches | ms/batch 87.85 | loss  0.03 | perplexity     1.03\n|  5600/22500 batches | ms/batch 85.64 | loss  0.03 | perplexity     1.03\n|  5800/22500 batches | ms/batch 85.70 | loss  0.03 | perplexity     1.03\n|  6000/22500 batches | ms/batch 85.69 | loss  0.03 | perplexity     1.03\n|  6200/22500 batches | ms/batch 85.76 | loss  0.03 | perplexity     1.03\n|  6400/22500 batches | ms/batch 85.61 | loss  0.04 | perplexity     1.04\n|  6600/22500 batches | ms/batch 87.87 | loss  0.02 | perplexity     1.02\n|  6800/22500 batches | ms/batch 85.71 | loss  0.03 | perplexity     1.03\n|  7000/22500 batches | ms/batch 85.81 | loss  0.19 | perplexity     1.21\n|  7200/22500 batches | ms/batch 85.63 | loss  0.12 | perplexity     1.13\n|  7400/22500 batches | ms/batch 85.71 | loss  0.02 | perplexity     1.02\n|  7600/22500 batches | ms/batch 85.79 | loss  0.02 | perplexity     1.02\n|  7800/22500 batches | ms/batch 87.69 | loss  0.02 | perplexity     1.02\n|  8000/22500 batches | ms/batch 85.65 | loss  0.02 | perplexity     1.02\n|  8200/22500 batches | ms/batch 85.73 | loss  0.07 | perplexity     1.07\n|  8400/22500 batches | ms/batch 85.77 | loss  0.01 | perplexity     1.02\n|  8600/22500 batches | ms/batch 86.01 | loss  0.01 | perplexity     1.01\n|  8800/22500 batches | ms/batch 87.16 | loss  0.02 | perplexity     1.02\n|  9000/22500 batches | ms/batch 88.43 | loss  0.02 | perplexity     1.02\n|  9200/22500 batches | ms/batch 85.83 | loss  0.01 | perplexity     1.01\n|  9400/22500 batches | ms/batch 85.72 | loss  0.02 | perplexity     1.02\n|  9600/22500 batches | ms/batch 86.02 | loss  0.01 | perplexity     1.01\n|  9800/22500 batches | ms/batch 85.83 | loss  0.02 | perplexity     1.02\n| 10000/22500 batches | ms/batch 85.87 | loss  0.02 | perplexity     1.02\n| 10200/22500 batches | ms/batch 87.81 | loss  0.06 | perplexity     1.06\n| 10400/22500 batches | ms/batch 85.71 | loss  0.01 | perplexity     1.01\n| 10600/22500 batches | ms/batch 85.89 | loss  0.01 | perplexity     1.01\n| 10800/22500 batches | ms/batch 85.71 | loss  0.01 | perplexity     1.01\n| 11000/22500 batches | ms/batch 85.76 | loss  0.01 | perplexity     1.01\n| 11200/22500 batches | ms/batch 85.80 | loss  0.01 | perplexity     1.01\n| 11400/22500 batches | ms/batch 87.89 | loss  0.01 | perplexity     1.01\n| 11600/22500 batches | ms/batch 85.65 | loss  0.01 | perplexity     1.01\n| 11800/22500 batches | ms/batch 85.77 | loss  0.02 | perplexity     1.02\n| 12000/22500 batches | ms/batch 85.69 | loss  0.01 | perplexity     1.01\n| 12200/22500 batches | ms/batch 85.71 | loss  0.01 | perplexity     1.01\n| 12400/22500 batches | ms/batch 85.71 | loss  0.01 | perplexity     1.01\n| 12600/22500 batches | ms/batch 87.85 | loss  0.01 | perplexity     1.01\n| 12800/22500 batches | ms/batch 85.84 | loss  0.01 | perplexity     1.01\n| 13000/22500 batches | ms/batch 85.78 | loss  0.12 | perplexity     1.13\n| 13200/22500 batches | ms/batch 85.90 | loss  0.01 | perplexity     1.01\n| 13400/22500 batches | ms/batch 85.80 | loss  0.01 | perplexity     1.01\n| 13600/22500 batches | ms/batch 85.76 | loss  0.01 | perplexity     1.01\n| 13800/22500 batches | ms/batch 87.84 | loss  0.01 | perplexity     1.01\n| 14000/22500 batches | ms/batch 85.83 | loss  0.01 | perplexity     1.01\n| 14200/22500 batches | ms/batch 85.74 | loss  0.01 | perplexity     1.01\n| 14400/22500 batches | ms/batch 85.78 | loss  0.01 | perplexity     1.01\n| 14600/22500 batches | ms/batch 85.83 | loss  0.01 | perplexity     1.01\n| 14800/22500 batches | ms/batch 85.72 | loss  0.01 | perplexity     1.01\n| 15000/22500 batches | ms/batch 87.81 | loss  0.01 | perplexity     1.01\n| 15200/22500 batches | ms/batch 85.68 | loss  0.01 | perplexity     1.01\n| 15400/22500 batches | ms/batch 85.73 | loss  0.01 | perplexity     1.01\n| 15600/22500 batches | ms/batch 85.67 | loss  0.00 | perplexity     1.00\n| 15800/22500 batches | ms/batch 85.86 | loss  0.01 | perplexity     1.01\n| 16000/22500 batches | ms/batch 85.81 | loss  0.01 | perplexity     1.01\n| 16200/22500 batches | ms/batch 87.67 | loss  0.03 | perplexity     1.03\n| 16400/22500 batches | ms/batch 85.70 | loss  0.01 | perplexity     1.01\n| 16600/22500 batches | ms/batch 85.89 | loss  0.01 | perplexity     1.01\n| 16800/22500 batches | ms/batch 85.73 | loss  0.00 | perplexity     1.00\n| 17000/22500 batches | ms/batch 85.80 | loss  0.00 | perplexity     1.00\n| 17200/22500 batches | ms/batch 85.75 | loss  0.01 | perplexity     1.01\n| 17400/22500 batches | ms/batch 87.92 | loss  0.00 | perplexity     1.00\n| 17600/22500 batches | ms/batch 85.76 | loss  0.01 | perplexity     1.01\n| 17800/22500 batches | ms/batch 85.70 | loss  0.01 | perplexity     1.01\n| 18000/22500 batches | ms/batch 85.72 | loss  0.01 | perplexity     1.01\n| 18200/22500 batches | ms/batch 85.68 | loss  0.01 | perplexity     1.01\n| 18400/22500 batches | ms/batch 85.78 | loss  0.01 | perplexity     1.01\n| 18600/22500 batches | ms/batch 87.75 | loss  0.00 | perplexity     1.00\n| 18800/22500 batches | ms/batch 85.75 | loss  0.01 | perplexity     1.01\n| 19000/22500 batches | ms/batch 85.67 | loss  0.01 | perplexity     1.01\n| 19200/22500 batches | ms/batch 85.76 | loss  0.01 | perplexity     1.01\n| 19400/22500 batches | ms/batch 85.78 | loss  0.07 | perplexity     1.07\n| 19600/22500 batches | ms/batch 85.78 | loss  0.01 | perplexity     1.01\n| 19800/22500 batches | ms/batch 87.96 | loss  0.00 | perplexity     1.00\n| 20000/22500 batches | ms/batch 85.67 | loss  0.01 | perplexity     1.01\n| 20200/22500 batches | ms/batch 85.82 | loss  0.01 | perplexity     1.01\n| 20400/22500 batches | ms/batch 85.84 | loss  0.00 | perplexity     1.00\n| 20600/22500 batches | ms/batch 85.87 | loss  0.00 | perplexity     1.00\n| 20800/22500 batches | ms/batch 85.77 | loss  0.01 | perplexity     1.01\n| 21000/22500 batches | ms/batch 87.89 | loss  0.01 | perplexity     1.01\n| 21200/22500 batches | ms/batch 85.75 | loss  0.01 | perplexity     1.01\n| 21400/22500 batches | ms/batch 85.74 | loss  0.01 | perplexity     1.01\n| 21600/22500 batches | ms/batch 85.71 | loss  0.01 | perplexity     1.01\n| 21800/22500 batches | ms/batch 85.81 | loss  0.00 | perplexity     1.00\n| 22000/22500 batches | ms/batch 85.76 | loss  0.00 | perplexity     1.00\n| 22200/22500 batches | ms/batch 85.81 | loss  0.01 | perplexity     1.01\n| 22400/22500 batches | ms/batch 87.78 | loss  0.00 | perplexity     1.00\n","output_type":"stream"}],"execution_count":null},{"id":"56d9d440","cell_type":"code","source":"model.eval()\n\nfor i in range(20):\n    prompt, answers = data_test[i]\n    prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n    output = generate(model, prompt_tensor, len(answers)).view((1,-1))\n    print(tokenizer.decode(output.tolist()[0]) + \"\\t actual result: \" + answers)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"56d9d440","outputId":"1872232b-b120-440b-e1a6-666e079efa3b","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T13:34:41.109443Z","iopub.execute_input":"2025-02-10T13:34:41.109793Z","iopub.status.idle":"2025-02-10T13:34:43.567274Z","shell.execute_reply.started":"2025-02-10T13:34:41.109754Z","shell.execute_reply":"2025-02-10T13:34:43.566485Z"}},"outputs":[{"name":"stdout","text":"1+0=6+4=3+0=7+0=5+8=6+8=3+0=5+9=3+5=944537401\t actual result: 944537401\n1+7=9+2=6+6=1+7=3+5=2+7=2+7=0+4=2+2=449989318\t actual result: 449989318\n8+8=7+5=9+4=1+8=3+4=8+4=5+6=5+4=7+6=1402280436\t actual result: 1402280436\n0+1=9+4=1+5=4+8=9+9=7+7=7+5=3+1=7+1=853592731\t actual result: 853592731\n8+6=9+9=0+9=4+8=1+9=7+3=0+9=4+6=5+1=710113094\t actual result: 710113094\n1+7=4+0=1+3=1+4=9+3=6+2=3+8=9+9=6+4=1191925448\t actual result: 1191925448\n6+9=2+5=4+5=1+7=7+9=4+8=1+1=6+3=2+7=993368985\t actual result: 993368985\n7+8=1+9=4+2=9+3=7+8=8+0=9+4=5+0=7+8=1563962715\t actual result: 1563962715\n9+2=8+6=1+6=3+6=8+0=4+8=0+0=2+0=4+3=721289851\t actual result: 721289851\n4+0=4+7=4+3=5+4=6+3=9+9=5+6=8+4=4+9=1432899814\t actual result: 1432899814\n1+6=8+6=1+4=1+6=6+6=0+6=7+8=1+8=7+4=1205727647\t actual result: 1205727647\n6+9=5+5=5+4=6+7=7+9=7+3=2+4=4+0=3+0=347174015\t actual result: 347174015\n6+8=5+6=0+7=1+3=6+7=5+4=1+3=6+1=9+5=1475034824\t actual result: 1475034824\n3+5=0+8=0+3=5+4=9+6=1+4=7+1=1+1=1+2=328659388\t actual result: 328659388\n3+8=1+0=0+6=2+5=9+5=0+1=3+0=5+4=4+7=1193247621\t actual result: 1193247621\n9+1=5+2=7+8=4+3=9+5=9+5=7+4=5+7=2+8=1132548580\t actual result: 1132548580\n0+3=2+6=7+7=7+6=8+8=5+3=8+5=1+4=2+6=863974483\t actual result: 863974483\n4+7=2+5=3+1=3+8=6+4=7+8=5+5=8+5=4+4=941611481\t actual result: 941611481\n4+2=8+7=6+8=9+5=4+0=6+3=6+7=7+7=4+0=553955556\t actual result: 553955556\n0+2=0+6=6+6=2+9=1+9=0+3=6+5=5+0=7+7=1461412262\t actual result: 1461412262\n","output_type":"stream"}],"execution_count":1},{"id":"qJ9IOZu8Xo4Y","cell_type":"markdown","source":"## Probing","metadata":{"id":"qJ9IOZu8Xo4Y"}},{"id":"78be1213","cell_type":"markdown","source":"This is just for fun...","metadata":{}},{"id":"yomPfirhXkLb","cell_type":"code","source":"import numpy as np\n\ntrain_size = 1000\ntest_size = 100\n\nmodel.eval()\n\ndef data_probing(size):\n    X = []\n    y = np.zeros(size)\n    for i in range(size):\n        input = torch.tensor(tokenizer.encode(data[i][0])).view((-1, 1)).to(device)\n        _, output = model(input)\n        output = output[-1,:,:].flatten()\n        # determine whether there was a carry in the result:\n        carry = len(data[i][1]) > len(data[i][0]) / 2\n        X.append(output.cpu().detach().numpy())\n        y[i] = carry\n    return np.array(X), y","metadata":{"id":"yomPfirhXkLb","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T13:35:04.361216Z","iopub.execute_input":"2025-02-10T13:35:04.361555Z","iopub.status.idle":"2025-02-10T13:35:04.367587Z","shell.execute_reply.started":"2025-02-10T13:35:04.361527Z","shell.execute_reply":"2025-02-10T13:35:04.366628Z"}},"outputs":[],"execution_count":3},{"id":"QGmfXVxkppfP","cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\nX_train, y_train = data_probing(train_size)\nX_test, y_test = data_probing(test_size)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)\n\nreg = LogisticRegression()\nreg.fit(X_train,y_train)\nreg.score(X_test, y_test)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QGmfXVxkppfP","outputId":"6601c884-004f-40bb-8a1a-71995b17d860","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T13:35:08.822759Z","iopub.execute_input":"2025-02-10T13:35:08.823063Z","iopub.status.idle":"2025-02-10T13:35:15.451772Z","shell.execute_reply.started":"2025-02-10T13:35:08.823043Z","shell.execute_reply":"2025-02-10T13:35:15.450992Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"1.0"},"metadata":{}}],"execution_count":4}]}