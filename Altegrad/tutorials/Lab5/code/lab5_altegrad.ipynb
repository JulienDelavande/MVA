{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "# Saisissez votre nom d'utilisateur GitHub\n",
        "username = 'JulienDelavande'\n",
        "\n",
        "# Saisissez votre PAT de manière sécurisée\n",
        "pat = getpass('Entrez votre Personal Access Token GitHub : ')\n",
        "!git clone https://{username}:{pat}@github.com/JulienDelavande/MVA.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ewhiom7x_azA",
        "outputId": "46442990-2ac2-471d-fd02-a53586fcf38f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrez votre Personal Access Token GitHub : ··········\n",
            "fatal: destination path 'MVA' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/MVA/Altegrad/tutorials/Lab5/code"
      ],
      "metadata": {
        "id": "lyVcZTkgc9rT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47daf1fb-011d-4416-cf67-c5bdc74a649f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MVA/Altegrad/tutorials/Lab5/code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r ./../../Lab4/code/RequirementsFiles/lab4-requirements.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-DqKnGPhAbR0",
        "outputId": "ddba2501-3405-4d95-9a57-b00bd43b595e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: click==8.1.7 in /usr/local/lib/python3.10/dist-packages (from -r ./../../Lab4/code/RequirementsFiles/lab4-requirements.txt (line 1)) (8.1.7)\n",
            "Collecting contourpy==1.3.0 (from -r ./../../Lab4/code/RequirementsFiles/lab4-requirements.txt (line 2))\n",
            "  Downloading contourpy-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.10/dist-packages (from -r ./../../Lab4/code/RequirementsFiles/lab4-requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: Cython==3.0.11 in /usr/local/lib/python3.10/dist-packages (from -r ./../../Lab4/code/RequirementsFiles/lab4-requirements.txt (line 4)) (3.0.11)\n",
            "Collecting fonttools==4.54.1 (from -r ./../../Lab4/code/RequirementsFiles/lab4-requirements.txt (line 5))\n",
            "  Downloading fonttools-4.54.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.7/163.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future==1.0.0 in /usr/local/lib/python3.10/dist-packages (from -r ./../../Lab4/code/RequirementsFiles/lab4-requirements.txt (line 6)) (1.0.0)\n",
            "Collecting GraKeL==0.1.10 (from -r ./../../Lab4/code/RequirementsFiles/lab4-requirements.txt (line 7))\n",
            "  Downloading GraKeL-0.1.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: importlib_resources==6.4.5 in /usr/local/lib/python3.10/dist-packages (from -r ./../../Lab4/code/RequirementsFiles/lab4-requirements.txt (line 8)) (6.4.5)\n",
            "Requirement already satisfied: joblib==1.4.2 in /usr/local/lib/python3.10/dist-packages (from -r ./../../Lab4/code/RequirementsFiles/lab4-requirements.txt (line 9)) (1.4.2)\n",
            "Requirement already satisfied: kiwisolver==1.4.7 in /usr/local/lib/python3.10/dist-packages (from -r ./../../Lab4/code/RequirementsFiles/lab4-requirements.txt (line 10)) (1.4.7)\n",
            "Collecting matplotlib==3.9.2 (from -r ./../../Lab4/code/RequirementsFiles/lab4-requirements.txt (line 11))\n",
            "  Downloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting networkx==3.2.1 (from -r ./../../Lab4/code/RequirementsFiles/lab4-requirements.txt (line 12))\n",
            "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: nltk==3.9.1 in /usr/local/lib/python3.10/dist-packages (from -r ./../../Lab4/code/RequirementsFiles/lab4-requirements.txt (line 13)) (3.9.1)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (from -r ./../../Lab4/code/RequirementsFiles/lab4-requirements.txt (line 14)) (1.26.4)\n",
            "Collecting packaging==24.1 (from -r ./../../Lab4/code/RequirementsFiles/lab4-requirements.txt (line 15))\n",
            "  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: pillow==11.0.0 in /usr/local/lib/python3.10/dist-packages (from -r ./../../Lab4/code/RequirementsFiles/lab4-requirements.txt (line 16)) (11.0.0)\n",
            "Requirement already satisfied: pyparsing==3.2.0 in /usr/local/lib/python3.10/dist-packages (from -r ./../../Lab4/code/RequirementsFiles/lab4-requirements.txt (line 17)) (3.2.0)\n",
            "Collecting python-dateutil==2.9.0.post0 (from -r ./../../Lab4/code/RequirementsFiles/lab4-requirements.txt (line 18))\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: regex==2024.9.11 in /usr/local/lib/python3.10/dist-packages (from -r ./../../Lab4/code/RequirementsFiles/lab4-requirements.txt (line 19)) (2024.9.11)\n",
            "Requirement already satisfied: scikit-learn==1.5.2 in /usr/local/lib/python3.10/dist-packages (from -r ./../../Lab4/code/RequirementsFiles/lab4-requirements.txt (line 20)) (1.5.2)\n",
            "Requirement already satisfied: scipy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from -r ./../../Lab4/code/RequirementsFiles/lab4-requirements.txt (line 21)) (1.13.1)\n",
            "Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.10/dist-packages (from -r ./../../Lab4/code/RequirementsFiles/lab4-requirements.txt (line 22)) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl==3.5.0 in /usr/local/lib/python3.10/dist-packages (from -r ./../../Lab4/code/RequirementsFiles/lab4-requirements.txt (line 23)) (3.5.0)\n",
            "Requirement already satisfied: tqdm==4.66.6 in /usr/local/lib/python3.10/dist-packages (from -r ./../../Lab4/code/RequirementsFiles/lab4-requirements.txt (line 24)) (4.66.6)\n",
            "Collecting zipp==3.20.2 (from -r ./../../Lab4/code/RequirementsFiles/lab4-requirements.txt (line 25))\n",
            "  Downloading zipp-3.20.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Downloading contourpy-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.0/322.0 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fonttools-4.54.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GraKeL-0.1.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-24.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zipp-3.20.2-py3-none-any.whl (9.2 kB)\n",
            "Installing collected packages: zipp, python-dateutil, packaging, networkx, fonttools, contourpy, matplotlib, GraKeL\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.21.0\n",
            "    Uninstalling zipp-3.21.0:\n",
            "      Successfully uninstalled zipp-3.21.0\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.4.2\n",
            "    Uninstalling networkx-3.4.2:\n",
            "      Successfully uninstalled networkx-3.4.2\n",
            "  Attempting uninstall: fonttools\n",
            "    Found existing installation: fonttools 4.55.0\n",
            "    Uninstalling fonttools-4.55.0:\n",
            "      Successfully uninstalled fonttools-4.55.0\n",
            "  Attempting uninstall: contourpy\n",
            "    Found existing installation: contourpy 1.3.1\n",
            "    Uninstalling contourpy-1.3.1:\n",
            "      Successfully uninstalled contourpy-1.3.1\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.8.0\n",
            "    Uninstalling matplotlib-3.8.0:\n",
            "      Successfully uninstalled matplotlib-3.8.0\n",
            "Successfully installed GraKeL-0.1.10 contourpy-1.3.0 fonttools-4.54.1 matplotlib-3.9.2 networkx-3.2.1 packaging-24.1 python-dateutil-2.9.0.post0 zipp-3.20.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil",
                  "zipp"
                ]
              },
              "id": "c4608c449a104fa5a33bc9a64f990b30"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install GraKeL\n",
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELhsO2MSdNCW",
        "outputId": "23c85769-1167-459f-8f25-852600d5e83e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: GraKeL in /usr/local/lib/python3.10/dist-packages (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from GraKeL) (1.26.4)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.10/dist-packages (from GraKeL) (3.0.11)\n",
            "Requirement already satisfied: scikit-learn>=0.19 in /usr/local/lib/python3.10/dist-packages (from GraKeL) (1.5.2)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from GraKeL) (1.16.0)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from GraKeL) (1.0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from GraKeL) (1.4.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19->GraKeL) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19->GraKeL) (3.5.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyxIEP8zCc9q",
        "outputId": "1d430d28-62bb-4563-9c4e-5bc07aafe099"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects:   6% (1/15)\u001b[K\rremote: Counting objects:  13% (2/15)\u001b[K\rremote: Counting objects:  20% (3/15)\u001b[K\rremote: Counting objects:  26% (4/15)\u001b[K\rremote: Counting objects:  33% (5/15)\u001b[K\rremote: Counting objects:  40% (6/15)\u001b[K\rremote: Counting objects:  46% (7/15)\u001b[K\rremote: Counting objects:  53% (8/15)\u001b[K\rremote: Counting objects:  60% (9/15)\u001b[K\rremote: Counting objects:  66% (10/15)\u001b[K\rremote: Counting objects:  73% (11/15)\u001b[K\rremote: Counting objects:  80% (12/15)\u001b[K\rremote: Counting objects:  86% (13/15)\u001b[K\rremote: Counting objects:  93% (14/15)\u001b[K\rremote: Counting objects: 100% (15/15)\u001b[K\rremote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects:  50% (1/2)\u001b[K\rremote: Compressing objects: 100% (2/2)\u001b[K\rremote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 8 (delta 6), reused 8 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects:  12% (1/8)\rUnpacking objects:  25% (2/8)\rUnpacking objects:  37% (3/8)\rUnpacking objects:  50% (4/8)\rUnpacking objects:  62% (5/8)\rUnpacking objects:  75% (6/8)\rUnpacking objects:  87% (7/8)\rUnpacking objects: 100% (8/8)\rUnpacking objects: 100% (8/8), 681 bytes | 340.00 KiB/s, done.\n",
            "From https://github.com/JulienDelavande/MVA\n",
            "   133c30c..51f6e2c  main       -> origin/main\n",
            "Updating 133c30c..51f6e2c\n",
            "Fast-forward\n",
            " Altegrad/tutorials/Lab5/code/part2/gnn_cora.py | 1 \u001b[32m+\u001b[m\n",
            " 1 file changed, 1 insertion(+)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python part1/visualization.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhM_UMKLEPJ_",
        "outputId": "b2a7e119-7538-419e-c67e-ee97b703af14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes: 33226\n",
            "Number of edges: 354529\n",
            "Generating walks\n",
            "Training word2vec\n",
            "Shape of node embeddings: (33226, 128)\n",
            "Figure(2000x1500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python part1/node_classification.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wak6z1o6N7qz",
        "outputId": "306ca5c9-bde6-4c5b-c9d2-daa637af33ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes: 34\n",
            "Number of edges: 78\n",
            "Figure(1000x1000)\n",
            "Generating walks\n",
            "Training word2vec\n",
            "Classification Accuracy: 1.0000\n",
            "DeepWalk Classification Accuracy: 1.0000\n",
            "Spectral Embeddings Classification Accuracy: 0.4286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python part2/gnn_karate.py"
      ],
      "metadata": {
        "id": "pjSc78iccu-P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62687947-e106-4946-b76d-fc6ecfdd055f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34\n",
            "78\n",
            "/content/MVA/Altegrad/tutorials/Lab5/code/part2/utils.py:54: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
            "  return torch.sparse.FloatTensor(indices, values, shape)\n",
            "Epoch: 001 loss_train: 0.6939 acc_train: 0.5556 time: 1.3894s\n",
            "Epoch: 002 loss_train: 0.6913 acc_train: 0.5556 time: 0.0049s\n",
            "Epoch: 003 loss_train: 0.6863 acc_train: 0.5556 time: 0.0071s\n",
            "Epoch: 004 loss_train: 0.6791 acc_train: 0.5556 time: 0.0044s\n",
            "Epoch: 005 loss_train: 0.6728 acc_train: 0.5556 time: 0.0037s\n",
            "Epoch: 006 loss_train: 0.6845 acc_train: 0.5926 time: 0.0043s\n",
            "Epoch: 007 loss_train: 0.6698 acc_train: 0.5926 time: 0.0048s\n",
            "Epoch: 008 loss_train: 0.6598 acc_train: 0.6296 time: 0.0051s\n",
            "Epoch: 009 loss_train: 0.6573 acc_train: 0.5926 time: 0.0048s\n",
            "Epoch: 010 loss_train: 0.6551 acc_train: 0.5926 time: 0.0053s\n",
            "Epoch: 011 loss_train: 0.6390 acc_train: 0.5926 time: 0.0049s\n",
            "Epoch: 012 loss_train: 0.6313 acc_train: 0.6667 time: 0.0039s\n",
            "Epoch: 013 loss_train: 0.6305 acc_train: 0.7037 time: 0.0038s\n",
            "Epoch: 014 loss_train: 0.6063 acc_train: 0.7037 time: 0.0038s\n",
            "Epoch: 015 loss_train: 0.5950 acc_train: 0.7407 time: 0.0035s\n",
            "Epoch: 016 loss_train: 0.5911 acc_train: 0.7407 time: 0.0035s\n",
            "Epoch: 017 loss_train: 0.5698 acc_train: 0.8519 time: 0.0037s\n",
            "Epoch: 018 loss_train: 0.5569 acc_train: 0.8519 time: 0.0036s\n",
            "Epoch: 019 loss_train: 0.5071 acc_train: 0.9259 time: 0.0038s\n",
            "Epoch: 020 loss_train: 0.4989 acc_train: 0.8889 time: 0.0047s\n",
            "Epoch: 021 loss_train: 0.4689 acc_train: 0.9259 time: 0.0039s\n",
            "Epoch: 022 loss_train: 0.4522 acc_train: 0.9630 time: 0.0038s\n",
            "Epoch: 023 loss_train: 0.4049 acc_train: 1.0000 time: 0.0035s\n",
            "Epoch: 024 loss_train: 0.4028 acc_train: 1.0000 time: 0.0035s\n",
            "Epoch: 025 loss_train: 0.3526 acc_train: 0.9630 time: 0.0038s\n",
            "Epoch: 026 loss_train: 0.3323 acc_train: 1.0000 time: 0.0036s\n",
            "Epoch: 027 loss_train: 0.3013 acc_train: 1.0000 time: 0.0036s\n",
            "Epoch: 028 loss_train: 0.2761 acc_train: 1.0000 time: 0.0039s\n",
            "Epoch: 029 loss_train: 0.2494 acc_train: 1.0000 time: 0.0036s\n",
            "Epoch: 030 loss_train: 0.2228 acc_train: 1.0000 time: 0.0037s\n",
            "Epoch: 031 loss_train: 0.2191 acc_train: 1.0000 time: 0.0037s\n",
            "Epoch: 032 loss_train: 0.1704 acc_train: 1.0000 time: 0.0037s\n",
            "Epoch: 033 loss_train: 0.1605 acc_train: 0.9630 time: 0.0036s\n",
            "Epoch: 034 loss_train: 0.1380 acc_train: 1.0000 time: 0.0034s\n",
            "Epoch: 035 loss_train: 0.1082 acc_train: 1.0000 time: 0.0035s\n",
            "Epoch: 036 loss_train: 0.1018 acc_train: 1.0000 time: 0.0036s\n",
            "Epoch: 037 loss_train: 0.0801 acc_train: 1.0000 time: 0.0040s\n",
            "Epoch: 038 loss_train: 0.0691 acc_train: 1.0000 time: 0.0037s\n",
            "Epoch: 039 loss_train: 0.0633 acc_train: 1.0000 time: 0.0036s\n",
            "Epoch: 040 loss_train: 0.0466 acc_train: 1.0000 time: 0.0037s\n",
            "Epoch: 041 loss_train: 0.0573 acc_train: 1.0000 time: 0.0042s\n",
            "Epoch: 042 loss_train: 0.0484 acc_train: 1.0000 time: 0.0041s\n",
            "Epoch: 043 loss_train: 0.0491 acc_train: 1.0000 time: 0.0041s\n",
            "Epoch: 044 loss_train: 0.0400 acc_train: 1.0000 time: 0.0043s\n",
            "Epoch: 045 loss_train: 0.0336 acc_train: 1.0000 time: 0.0044s\n",
            "Epoch: 046 loss_train: 0.0404 acc_train: 1.0000 time: 0.0044s\n",
            "Epoch: 047 loss_train: 0.0540 acc_train: 1.0000 time: 0.0046s\n",
            "Epoch: 048 loss_train: 0.0418 acc_train: 1.0000 time: 0.0051s\n",
            "Epoch: 049 loss_train: 0.0112 acc_train: 1.0000 time: 0.0045s\n",
            "Epoch: 050 loss_train: 0.0211 acc_train: 1.0000 time: 0.0042s\n",
            "Epoch: 051 loss_train: 0.0146 acc_train: 1.0000 time: 0.0035s\n",
            "Epoch: 052 loss_train: 0.0267 acc_train: 1.0000 time: 0.0062s\n",
            "Epoch: 053 loss_train: 0.0073 acc_train: 1.0000 time: 0.0075s\n",
            "Epoch: 054 loss_train: 0.0100 acc_train: 1.0000 time: 0.0044s\n",
            "Epoch: 055 loss_train: 0.0099 acc_train: 1.0000 time: 0.0036s\n",
            "Epoch: 056 loss_train: 0.0145 acc_train: 1.0000 time: 0.0037s\n",
            "Epoch: 057 loss_train: 0.0059 acc_train: 1.0000 time: 0.0040s\n",
            "Epoch: 058 loss_train: 0.0106 acc_train: 1.0000 time: 0.0038s\n",
            "Epoch: 059 loss_train: 0.0062 acc_train: 1.0000 time: 0.0036s\n",
            "Epoch: 060 loss_train: 0.0067 acc_train: 1.0000 time: 0.0036s\n",
            "Epoch: 061 loss_train: 0.0084 acc_train: 1.0000 time: 0.0037s\n",
            "Epoch: 062 loss_train: 0.0128 acc_train: 1.0000 time: 0.0034s\n",
            "Epoch: 063 loss_train: 0.0133 acc_train: 1.0000 time: 0.0037s\n",
            "Epoch: 064 loss_train: 0.0035 acc_train: 1.0000 time: 0.0038s\n",
            "Epoch: 065 loss_train: 0.0042 acc_train: 1.0000 time: 0.0035s\n",
            "Epoch: 066 loss_train: 0.0042 acc_train: 1.0000 time: 0.0043s\n",
            "Epoch: 067 loss_train: 0.0039 acc_train: 1.0000 time: 0.0037s\n",
            "Epoch: 068 loss_train: 0.0087 acc_train: 1.0000 time: 0.0038s\n",
            "Epoch: 069 loss_train: 0.0029 acc_train: 1.0000 time: 0.0036s\n",
            "Epoch: 070 loss_train: 0.0038 acc_train: 1.0000 time: 0.0035s\n",
            "Epoch: 071 loss_train: 0.0042 acc_train: 1.0000 time: 0.0035s\n",
            "Epoch: 072 loss_train: 0.0153 acc_train: 1.0000 time: 0.0037s\n",
            "Epoch: 073 loss_train: 0.0297 acc_train: 0.9630 time: 0.0036s\n",
            "Epoch: 074 loss_train: 0.0044 acc_train: 1.0000 time: 0.0036s\n",
            "Epoch: 075 loss_train: 0.0067 acc_train: 1.0000 time: 0.0036s\n",
            "Epoch: 076 loss_train: 0.0267 acc_train: 1.0000 time: 0.0036s\n",
            "Epoch: 077 loss_train: 0.0059 acc_train: 1.0000 time: 0.0036s\n",
            "Epoch: 078 loss_train: 0.0015 acc_train: 1.0000 time: 0.0038s\n",
            "Epoch: 079 loss_train: 0.0160 acc_train: 1.0000 time: 0.0043s\n",
            "Epoch: 080 loss_train: 0.0060 acc_train: 1.0000 time: 0.0039s\n",
            "Epoch: 081 loss_train: 0.0018 acc_train: 1.0000 time: 0.0039s\n",
            "Epoch: 082 loss_train: 0.0253 acc_train: 1.0000 time: 0.0050s\n",
            "Epoch: 083 loss_train: 0.0019 acc_train: 1.0000 time: 0.0039s\n",
            "Epoch: 084 loss_train: 0.0016 acc_train: 1.0000 time: 0.0039s\n",
            "Epoch: 085 loss_train: 0.0061 acc_train: 1.0000 time: 0.0038s\n",
            "Epoch: 086 loss_train: 0.0054 acc_train: 1.0000 time: 0.0038s\n",
            "Epoch: 087 loss_train: 0.0099 acc_train: 1.0000 time: 0.0039s\n",
            "Epoch: 088 loss_train: 0.0027 acc_train: 1.0000 time: 0.0039s\n",
            "Epoch: 089 loss_train: 0.0055 acc_train: 1.0000 time: 0.0038s\n",
            "Epoch: 090 loss_train: 0.0025 acc_train: 1.0000 time: 0.0039s\n",
            "Epoch: 091 loss_train: 0.0021 acc_train: 1.0000 time: 0.0037s\n",
            "Epoch: 092 loss_train: 0.0025 acc_train: 1.0000 time: 0.0038s\n",
            "Epoch: 093 loss_train: 0.0027 acc_train: 1.0000 time: 0.0036s\n",
            "Epoch: 094 loss_train: 0.0023 acc_train: 1.0000 time: 0.0036s\n",
            "Epoch: 095 loss_train: 0.0054 acc_train: 1.0000 time: 0.0037s\n",
            "Epoch: 096 loss_train: 0.0152 acc_train: 1.0000 time: 0.0039s\n",
            "Epoch: 097 loss_train: 0.0118 acc_train: 1.0000 time: 0.0036s\n",
            "Epoch: 098 loss_train: 0.0066 acc_train: 1.0000 time: 0.0036s\n",
            "Epoch: 099 loss_train: 0.0011 acc_train: 1.0000 time: 0.0035s\n",
            "Epoch: 100 loss_train: 0.0020 acc_train: 1.0000 time: 0.0035s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 1.7898s\n",
            "\n",
            "Test set results: loss= 0.0001 accuracy= 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python part2/gnn_karate.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_eLA-Uakt_2",
        "outputId": "cd9f2773-ac75-4965-8535-59a121a43063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34\n",
            "78\n",
            "/content/MVA/Altegrad/tutorials/Lab5/code/part2/utils.py:54: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
            "  return torch.sparse.FloatTensor(indices, values, shape)\n",
            "Epoch: 001 loss_train: 0.6909 acc_train: 0.4074 time: 0.8822s\n",
            "Epoch: 002 loss_train: 0.7011 acc_train: 0.5185 time: 0.0045s\n",
            "Epoch: 003 loss_train: 0.6985 acc_train: 0.5185 time: 0.0041s\n",
            "Epoch: 004 loss_train: 0.6936 acc_train: 0.5185 time: 0.0035s\n",
            "Epoch: 005 loss_train: 0.7013 acc_train: 0.4444 time: 0.0035s\n",
            "Epoch: 006 loss_train: 0.6842 acc_train: 0.5556 time: 0.0043s\n",
            "Epoch: 007 loss_train: 0.6797 acc_train: 0.5556 time: 0.0037s\n",
            "Epoch: 008 loss_train: 0.6924 acc_train: 0.5556 time: 0.0035s\n",
            "Epoch: 009 loss_train: 0.6976 acc_train: 0.5556 time: 0.0035s\n",
            "Epoch: 010 loss_train: 0.7022 acc_train: 0.5556 time: 0.0036s\n",
            "Epoch: 011 loss_train: 0.6933 acc_train: 0.5556 time: 0.0034s\n",
            "Epoch: 012 loss_train: 0.6893 acc_train: 0.5556 time: 0.0036s\n",
            "Epoch: 013 loss_train: 0.6979 acc_train: 0.5556 time: 0.0035s\n",
            "Epoch: 014 loss_train: 0.6838 acc_train: 0.5556 time: 0.0036s\n",
            "Epoch: 015 loss_train: 0.6841 acc_train: 0.5556 time: 0.0035s\n",
            "Epoch: 016 loss_train: 0.6930 acc_train: 0.5556 time: 0.0036s\n",
            "Epoch: 017 loss_train: 0.6925 acc_train: 0.5556 time: 0.0034s\n",
            "Epoch: 018 loss_train: 0.6939 acc_train: 0.5556 time: 0.0034s\n",
            "Epoch: 019 loss_train: 0.6892 acc_train: 0.5556 time: 0.0037s\n",
            "Epoch: 020 loss_train: 0.6947 acc_train: 0.5556 time: 0.0036s\n",
            "Epoch: 021 loss_train: 0.6890 acc_train: 0.5556 time: 0.0036s\n",
            "Epoch: 022 loss_train: 0.6931 acc_train: 0.5556 time: 0.0036s\n",
            "Epoch: 023 loss_train: 0.6842 acc_train: 0.5556 time: 0.0034s\n",
            "Epoch: 024 loss_train: 0.6845 acc_train: 0.5556 time: 0.0034s\n",
            "Epoch: 025 loss_train: 0.6858 acc_train: 0.5556 time: 0.0039s\n",
            "Epoch: 026 loss_train: 0.6879 acc_train: 0.5556 time: 0.0035s\n",
            "Epoch: 027 loss_train: 0.6849 acc_train: 0.5556 time: 0.0036s\n",
            "Epoch: 028 loss_train: 0.6889 acc_train: 0.5556 time: 0.0037s\n",
            "Epoch: 029 loss_train: 0.6926 acc_train: 0.5556 time: 0.0033s\n",
            "Epoch: 030 loss_train: 0.6858 acc_train: 0.5556 time: 0.0035s\n",
            "Epoch: 031 loss_train: 0.6849 acc_train: 0.5556 time: 0.0037s\n",
            "Epoch: 032 loss_train: 0.6844 acc_train: 0.5556 time: 0.0034s\n",
            "Epoch: 033 loss_train: 0.6871 acc_train: 0.5556 time: 0.0033s\n",
            "Epoch: 034 loss_train: 0.6868 acc_train: 0.5556 time: 0.0035s\n",
            "Epoch: 035 loss_train: 0.6897 acc_train: 0.5556 time: 0.0033s\n",
            "Epoch: 036 loss_train: 0.6865 acc_train: 0.5556 time: 0.0037s\n",
            "Epoch: 037 loss_train: 0.6863 acc_train: 0.5556 time: 0.0037s\n",
            "Epoch: 038 loss_train: 0.6837 acc_train: 0.5556 time: 0.0034s\n",
            "Epoch: 039 loss_train: 0.6823 acc_train: 0.5556 time: 0.0037s\n",
            "Epoch: 040 loss_train: 0.6853 acc_train: 0.5556 time: 0.0033s\n",
            "Epoch: 041 loss_train: 0.6826 acc_train: 0.5556 time: 0.0034s\n",
            "Epoch: 042 loss_train: 0.6889 acc_train: 0.5556 time: 0.0034s\n",
            "Epoch: 043 loss_train: 0.6866 acc_train: 0.5556 time: 0.0034s\n",
            "Epoch: 044 loss_train: 0.6926 acc_train: 0.5556 time: 0.0034s\n",
            "Epoch: 045 loss_train: 0.6900 acc_train: 0.5556 time: 0.0036s\n",
            "Epoch: 046 loss_train: 0.6832 acc_train: 0.5556 time: 0.0036s\n",
            "Epoch: 047 loss_train: 0.6898 acc_train: 0.5556 time: 0.0035s\n",
            "Epoch: 048 loss_train: 0.6887 acc_train: 0.5556 time: 0.0035s\n",
            "Epoch: 049 loss_train: 0.6879 acc_train: 0.5556 time: 0.0035s\n",
            "Epoch: 050 loss_train: 0.6887 acc_train: 0.5556 time: 0.0035s\n",
            "Epoch: 051 loss_train: 0.6810 acc_train: 0.5556 time: 0.0035s\n",
            "Epoch: 052 loss_train: 0.6939 acc_train: 0.5556 time: 0.0034s\n",
            "Epoch: 053 loss_train: 0.6861 acc_train: 0.5556 time: 0.0034s\n",
            "Epoch: 054 loss_train: 0.6866 acc_train: 0.5556 time: 0.0035s\n",
            "Epoch: 055 loss_train: 0.6931 acc_train: 0.5556 time: 0.0035s\n",
            "Epoch: 056 loss_train: 0.6886 acc_train: 0.5556 time: 0.0033s\n",
            "Epoch: 057 loss_train: 0.6819 acc_train: 0.5556 time: 0.0033s\n",
            "Epoch: 058 loss_train: 0.6900 acc_train: 0.5556 time: 0.0034s\n",
            "Epoch: 059 loss_train: 0.6862 acc_train: 0.5556 time: 0.0041s\n",
            "Epoch: 060 loss_train: 0.6847 acc_train: 0.5556 time: 0.0056s\n",
            "Epoch: 061 loss_train: 0.6884 acc_train: 0.5556 time: 0.0035s\n",
            "Epoch: 062 loss_train: 0.6875 acc_train: 0.5556 time: 0.0036s\n",
            "Epoch: 063 loss_train: 0.6902 acc_train: 0.5556 time: 0.0036s\n",
            "Epoch: 064 loss_train: 0.6833 acc_train: 0.5556 time: 0.0034s\n",
            "Epoch: 065 loss_train: 0.6900 acc_train: 0.5556 time: 0.0034s\n",
            "Epoch: 066 loss_train: 0.6921 acc_train: 0.5556 time: 0.0034s\n",
            "Epoch: 067 loss_train: 0.6862 acc_train: 0.5556 time: 0.0033s\n",
            "Epoch: 068 loss_train: 0.6946 acc_train: 0.5556 time: 0.0034s\n",
            "Epoch: 069 loss_train: 0.6897 acc_train: 0.5556 time: 0.0034s\n",
            "Epoch: 070 loss_train: 0.6926 acc_train: 0.5556 time: 0.0034s\n",
            "Epoch: 071 loss_train: 0.6903 acc_train: 0.5556 time: 0.0034s\n",
            "Epoch: 072 loss_train: 0.6913 acc_train: 0.5556 time: 0.0034s\n",
            "Epoch: 073 loss_train: 0.6871 acc_train: 0.5556 time: 0.0034s\n",
            "Epoch: 074 loss_train: 0.6887 acc_train: 0.5556 time: 0.0037s\n",
            "Epoch: 075 loss_train: 0.6833 acc_train: 0.5556 time: 0.0035s\n",
            "Epoch: 076 loss_train: 0.6905 acc_train: 0.5556 time: 0.0036s\n",
            "Epoch: 077 loss_train: 0.6909 acc_train: 0.5556 time: 0.0036s\n",
            "Epoch: 078 loss_train: 0.6885 acc_train: 0.5556 time: 0.0044s\n",
            "Epoch: 079 loss_train: 0.6872 acc_train: 0.5556 time: 0.0035s\n",
            "Epoch: 080 loss_train: 0.6885 acc_train: 0.5556 time: 0.0036s\n",
            "Epoch: 081 loss_train: 0.6888 acc_train: 0.5556 time: 0.0035s\n",
            "Epoch: 082 loss_train: 0.6893 acc_train: 0.5556 time: 0.0036s\n",
            "Epoch: 083 loss_train: 0.6860 acc_train: 0.5556 time: 0.0036s\n",
            "Epoch: 084 loss_train: 0.6876 acc_train: 0.5556 time: 0.0035s\n",
            "Epoch: 085 loss_train: 0.6845 acc_train: 0.5556 time: 0.0036s\n",
            "Epoch: 086 loss_train: 0.6862 acc_train: 0.5556 time: 0.0036s\n",
            "Epoch: 087 loss_train: 0.6852 acc_train: 0.5556 time: 0.0035s\n",
            "Epoch: 088 loss_train: 0.6896 acc_train: 0.5556 time: 0.0035s\n",
            "Epoch: 089 loss_train: 0.6873 acc_train: 0.5556 time: 0.0037s\n",
            "Epoch: 090 loss_train: 0.6884 acc_train: 0.5556 time: 0.0037s\n",
            "Epoch: 091 loss_train: 0.6854 acc_train: 0.5556 time: 0.0037s\n",
            "Epoch: 092 loss_train: 0.6841 acc_train: 0.5556 time: 0.0037s\n",
            "Epoch: 093 loss_train: 0.6827 acc_train: 0.5556 time: 0.0036s\n",
            "Epoch: 094 loss_train: 0.6846 acc_train: 0.5556 time: 0.0035s\n",
            "Epoch: 095 loss_train: 0.6879 acc_train: 0.5556 time: 0.0035s\n",
            "Epoch: 096 loss_train: 0.6859 acc_train: 0.5556 time: 0.0034s\n",
            "Epoch: 097 loss_train: 0.6888 acc_train: 0.5556 time: 0.0035s\n",
            "Epoch: 098 loss_train: 0.6845 acc_train: 0.5556 time: 0.0035s\n",
            "Epoch: 099 loss_train: 0.6874 acc_train: 0.5556 time: 0.0036s\n",
            "Epoch: 100 loss_train: 0.6841 acc_train: 0.5556 time: 0.0036s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 1.2408s\n",
            "\n",
            "Test set results: loss= 0.7156 accuracy= 0.4286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python part2/gnn_cora.py"
      ],
      "metadata": {
        "id": "QVsP5mUcmUhT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c7ae505-f21d-4089-b291-3d462e283927"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset has 2708 nodes, 5429 edges, 1433 features.\n",
            "/content/MVA/Altegrad/tutorials/Lab5/code/part2/utils.py:54: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
            "  return torch.sparse.FloatTensor(indices, values, shape)\n",
            "Epoch: 001 loss_train: 1.9194 acc_train: 0.1533 loss_val: 1.9025 acc_val: 0.2638 time: 0.8831s\n",
            "Epoch: 002 loss_train: 1.9085 acc_train: 0.2340 loss_val: 1.8919 acc_val: 0.2952 time: 0.0115s\n",
            "Epoch: 003 loss_train: 1.8965 acc_train: 0.3054 loss_val: 1.8819 acc_val: 0.2952 time: 0.0111s\n",
            "Epoch: 004 loss_train: 1.8888 acc_train: 0.3036 loss_val: 1.8720 acc_val: 0.2952 time: 0.0108s\n",
            "Epoch: 005 loss_train: 1.8797 acc_train: 0.3036 loss_val: 1.8601 acc_val: 0.2952 time: 0.0107s\n",
            "Epoch: 006 loss_train: 1.8707 acc_train: 0.3036 loss_val: 1.8451 acc_val: 0.2952 time: 0.0108s\n",
            "Epoch: 007 loss_train: 1.8558 acc_train: 0.3036 loss_val: 1.8286 acc_val: 0.2952 time: 0.0108s\n",
            "Epoch: 008 loss_train: 1.8417 acc_train: 0.3036 loss_val: 1.8106 acc_val: 0.2952 time: 0.0110s\n",
            "Epoch: 009 loss_train: 1.8232 acc_train: 0.3036 loss_val: 1.7929 acc_val: 0.2952 time: 0.0106s\n",
            "Epoch: 010 loss_train: 1.8199 acc_train: 0.3036 loss_val: 1.7774 acc_val: 0.2952 time: 0.0108s\n",
            "Epoch: 011 loss_train: 1.8065 acc_train: 0.3036 loss_val: 1.7633 acc_val: 0.2952 time: 0.0106s\n",
            "Epoch: 012 loss_train: 1.8026 acc_train: 0.3036 loss_val: 1.7470 acc_val: 0.2952 time: 0.0109s\n",
            "Epoch: 013 loss_train: 1.7755 acc_train: 0.3036 loss_val: 1.7275 acc_val: 0.2952 time: 0.0106s\n",
            "Epoch: 014 loss_train: 1.7632 acc_train: 0.3036 loss_val: 1.7076 acc_val: 0.2952 time: 0.0106s\n",
            "Epoch: 015 loss_train: 1.7487 acc_train: 0.3067 loss_val: 1.6879 acc_val: 0.2952 time: 0.0108s\n",
            "Epoch: 016 loss_train: 1.7086 acc_train: 0.3054 loss_val: 1.6656 acc_val: 0.2952 time: 0.0108s\n",
            "Epoch: 017 loss_train: 1.6881 acc_train: 0.3110 loss_val: 1.6374 acc_val: 0.2952 time: 0.0108s\n",
            "Epoch: 018 loss_train: 1.6657 acc_train: 0.3171 loss_val: 1.6020 acc_val: 0.2952 time: 0.0110s\n",
            "Epoch: 019 loss_train: 1.6227 acc_train: 0.3264 loss_val: 1.5593 acc_val: 0.2989 time: 0.0110s\n",
            "Epoch: 020 loss_train: 1.5832 acc_train: 0.3479 loss_val: 1.5140 acc_val: 0.3100 time: 0.0110s\n",
            "Epoch: 021 loss_train: 1.5471 acc_train: 0.3578 loss_val: 1.4693 acc_val: 0.3339 time: 0.0107s\n",
            "Epoch: 022 loss_train: 1.5038 acc_train: 0.3658 loss_val: 1.4261 acc_val: 0.3690 time: 0.0107s\n",
            "Epoch: 023 loss_train: 1.4746 acc_train: 0.3793 loss_val: 1.3837 acc_val: 0.3967 time: 0.0106s\n",
            "Epoch: 024 loss_train: 1.4274 acc_train: 0.4119 loss_val: 1.3420 acc_val: 0.4096 time: 0.0106s\n",
            "Epoch: 025 loss_train: 1.3900 acc_train: 0.4433 loss_val: 1.3006 acc_val: 0.4631 time: 0.0101s\n",
            "Epoch: 026 loss_train: 1.3480 acc_train: 0.4748 loss_val: 1.2578 acc_val: 0.5332 time: 0.0096s\n",
            "Epoch: 027 loss_train: 1.3008 acc_train: 0.5086 loss_val: 1.2111 acc_val: 0.5923 time: 0.0095s\n",
            "Epoch: 028 loss_train: 1.2577 acc_train: 0.5351 loss_val: 1.1635 acc_val: 0.6144 time: 0.0094s\n",
            "Epoch: 029 loss_train: 1.2290 acc_train: 0.5339 loss_val: 1.1181 acc_val: 0.6292 time: 0.0103s\n",
            "Epoch: 030 loss_train: 1.1964 acc_train: 0.5382 loss_val: 1.0756 acc_val: 0.6531 time: 0.0099s\n",
            "Epoch: 031 loss_train: 1.1502 acc_train: 0.5665 loss_val: 1.0369 acc_val: 0.6642 time: 0.0093s\n",
            "Epoch: 032 loss_train: 1.1170 acc_train: 0.5819 loss_val: 1.0007 acc_val: 0.6642 time: 0.0093s\n",
            "Epoch: 033 loss_train: 1.0864 acc_train: 0.6004 loss_val: 0.9666 acc_val: 0.6734 time: 0.0098s\n",
            "Epoch: 034 loss_train: 1.0550 acc_train: 0.6047 loss_val: 0.9345 acc_val: 0.6808 time: 0.0094s\n",
            "Epoch: 035 loss_train: 1.0442 acc_train: 0.6108 loss_val: 0.9069 acc_val: 0.6808 time: 0.0095s\n",
            "Epoch: 036 loss_train: 1.0201 acc_train: 0.6145 loss_val: 0.8853 acc_val: 0.6882 time: 0.0094s\n",
            "Epoch: 037 loss_train: 0.9729 acc_train: 0.6342 loss_val: 0.8618 acc_val: 0.7103 time: 0.0097s\n",
            "Epoch: 038 loss_train: 0.9639 acc_train: 0.6379 loss_val: 0.8361 acc_val: 0.7325 time: 0.0096s\n",
            "Epoch: 039 loss_train: 0.9271 acc_train: 0.6656 loss_val: 0.8148 acc_val: 0.7509 time: 0.0094s\n",
            "Epoch: 040 loss_train: 0.8990 acc_train: 0.6798 loss_val: 0.7973 acc_val: 0.7620 time: 0.0093s\n",
            "Epoch: 041 loss_train: 0.9048 acc_train: 0.6724 loss_val: 0.7836 acc_val: 0.7620 time: 0.0093s\n",
            "Epoch: 042 loss_train: 0.8714 acc_train: 0.6903 loss_val: 0.7694 acc_val: 0.7694 time: 0.0093s\n",
            "Epoch: 043 loss_train: 0.8468 acc_train: 0.7026 loss_val: 0.7563 acc_val: 0.7675 time: 0.0094s\n",
            "Epoch: 044 loss_train: 0.8168 acc_train: 0.7137 loss_val: 0.7415 acc_val: 0.7694 time: 0.0096s\n",
            "Epoch: 045 loss_train: 0.8032 acc_train: 0.7131 loss_val: 0.7268 acc_val: 0.7712 time: 0.0096s\n",
            "Epoch: 046 loss_train: 0.7970 acc_train: 0.7186 loss_val: 0.7128 acc_val: 0.7731 time: 0.0095s\n",
            "Epoch: 047 loss_train: 0.7830 acc_train: 0.7248 loss_val: 0.7008 acc_val: 0.7749 time: 0.0090s\n",
            "Epoch: 048 loss_train: 0.7419 acc_train: 0.7272 loss_val: 0.6913 acc_val: 0.7749 time: 0.0092s\n",
            "Epoch: 049 loss_train: 0.7566 acc_train: 0.7260 loss_val: 0.6844 acc_val: 0.7768 time: 0.0090s\n",
            "Epoch: 050 loss_train: 0.7291 acc_train: 0.7414 loss_val: 0.6762 acc_val: 0.7804 time: 0.0092s\n",
            "Epoch: 051 loss_train: 0.6958 acc_train: 0.7451 loss_val: 0.6664 acc_val: 0.7804 time: 0.0091s\n",
            "Epoch: 052 loss_train: 0.6934 acc_train: 0.7605 loss_val: 0.6575 acc_val: 0.7897 time: 0.0092s\n",
            "Epoch: 053 loss_train: 0.6784 acc_train: 0.7494 loss_val: 0.6468 acc_val: 0.7915 time: 0.0090s\n",
            "Epoch: 054 loss_train: 0.6867 acc_train: 0.7660 loss_val: 0.6376 acc_val: 0.7878 time: 0.0091s\n",
            "Epoch: 055 loss_train: 0.6538 acc_train: 0.7635 loss_val: 0.6314 acc_val: 0.7989 time: 0.0090s\n",
            "Epoch: 056 loss_train: 0.6649 acc_train: 0.7623 loss_val: 0.6271 acc_val: 0.8100 time: 0.0091s\n",
            "Epoch: 057 loss_train: 0.6271 acc_train: 0.7943 loss_val: 0.6247 acc_val: 0.8210 time: 0.0090s\n",
            "Epoch: 058 loss_train: 0.6215 acc_train: 0.7894 loss_val: 0.6186 acc_val: 0.8266 time: 0.0097s\n",
            "Epoch: 059 loss_train: 0.6126 acc_train: 0.7925 loss_val: 0.6115 acc_val: 0.8376 time: 0.0111s\n",
            "Epoch: 060 loss_train: 0.5964 acc_train: 0.8134 loss_val: 0.6037 acc_val: 0.8321 time: 0.0112s\n",
            "Epoch: 061 loss_train: 0.6151 acc_train: 0.7919 loss_val: 0.5965 acc_val: 0.8358 time: 0.0104s\n",
            "Epoch: 062 loss_train: 0.5820 acc_train: 0.7968 loss_val: 0.5908 acc_val: 0.8524 time: 0.0100s\n",
            "Epoch: 063 loss_train: 0.5628 acc_train: 0.8091 loss_val: 0.5900 acc_val: 0.8635 time: 0.0104s\n",
            "Epoch: 064 loss_train: 0.5632 acc_train: 0.8116 loss_val: 0.5912 acc_val: 0.8598 time: 0.0089s\n",
            "Epoch: 065 loss_train: 0.5538 acc_train: 0.8159 loss_val: 0.5904 acc_val: 0.8598 time: 0.0090s\n",
            "Epoch: 066 loss_train: 0.5401 acc_train: 0.8128 loss_val: 0.5812 acc_val: 0.8635 time: 0.0090s\n",
            "Epoch: 067 loss_train: 0.5255 acc_train: 0.8171 loss_val: 0.5720 acc_val: 0.8635 time: 0.0089s\n",
            "Epoch: 068 loss_train: 0.5162 acc_train: 0.8202 loss_val: 0.5677 acc_val: 0.8727 time: 0.0088s\n",
            "Epoch: 069 loss_train: 0.4980 acc_train: 0.8282 loss_val: 0.5630 acc_val: 0.8745 time: 0.0092s\n",
            "Epoch: 070 loss_train: 0.5286 acc_train: 0.8325 loss_val: 0.5577 acc_val: 0.8819 time: 0.0096s\n",
            "Epoch: 071 loss_train: 0.5032 acc_train: 0.8319 loss_val: 0.5560 acc_val: 0.8801 time: 0.0095s\n",
            "Epoch: 072 loss_train: 0.4908 acc_train: 0.8405 loss_val: 0.5574 acc_val: 0.8819 time: 0.0090s\n",
            "Epoch: 073 loss_train: 0.4908 acc_train: 0.8393 loss_val: 0.5574 acc_val: 0.8819 time: 0.0093s\n",
            "Epoch: 074 loss_train: 0.4863 acc_train: 0.8424 loss_val: 0.5491 acc_val: 0.8838 time: 0.0091s\n",
            "Epoch: 075 loss_train: 0.4627 acc_train: 0.8461 loss_val: 0.5419 acc_val: 0.8856 time: 0.0092s\n",
            "Epoch: 076 loss_train: 0.4797 acc_train: 0.8461 loss_val: 0.5355 acc_val: 0.8819 time: 0.0090s\n",
            "Epoch: 077 loss_train: 0.4700 acc_train: 0.8417 loss_val: 0.5302 acc_val: 0.8856 time: 0.0094s\n",
            "Epoch: 078 loss_train: 0.4541 acc_train: 0.8633 loss_val: 0.5264 acc_val: 0.8838 time: 0.0090s\n",
            "Epoch: 079 loss_train: 0.4603 acc_train: 0.8516 loss_val: 0.5264 acc_val: 0.8856 time: 0.0092s\n",
            "Epoch: 080 loss_train: 0.4385 acc_train: 0.8602 loss_val: 0.5280 acc_val: 0.8875 time: 0.0092s\n",
            "Epoch: 081 loss_train: 0.4425 acc_train: 0.8633 loss_val: 0.5267 acc_val: 0.8838 time: 0.0101s\n",
            "Epoch: 082 loss_train: 0.4223 acc_train: 0.8682 loss_val: 0.5211 acc_val: 0.8838 time: 0.0093s\n",
            "Epoch: 083 loss_train: 0.4035 acc_train: 0.8725 loss_val: 0.5182 acc_val: 0.8819 time: 0.0091s\n",
            "Epoch: 084 loss_train: 0.3973 acc_train: 0.8707 loss_val: 0.5125 acc_val: 0.8838 time: 0.0091s\n",
            "Epoch: 085 loss_train: 0.4061 acc_train: 0.8615 loss_val: 0.5090 acc_val: 0.8838 time: 0.0090s\n",
            "Epoch: 086 loss_train: 0.3942 acc_train: 0.8707 loss_val: 0.5059 acc_val: 0.8893 time: 0.0098s\n",
            "Epoch: 087 loss_train: 0.3932 acc_train: 0.8676 loss_val: 0.5075 acc_val: 0.8911 time: 0.0091s\n",
            "Epoch: 088 loss_train: 0.3694 acc_train: 0.8793 loss_val: 0.5113 acc_val: 0.8875 time: 0.0091s\n",
            "Epoch: 089 loss_train: 0.3950 acc_train: 0.8744 loss_val: 0.5163 acc_val: 0.8838 time: 0.0089s\n",
            "Epoch: 090 loss_train: 0.3789 acc_train: 0.8793 loss_val: 0.5178 acc_val: 0.8856 time: 0.0090s\n",
            "Epoch: 091 loss_train: 0.3811 acc_train: 0.8799 loss_val: 0.5114 acc_val: 0.8893 time: 0.0090s\n",
            "Epoch: 092 loss_train: 0.3694 acc_train: 0.8781 loss_val: 0.5048 acc_val: 0.8856 time: 0.0092s\n",
            "Epoch: 093 loss_train: 0.3555 acc_train: 0.8861 loss_val: 0.5024 acc_val: 0.8819 time: 0.0100s\n",
            "Epoch: 094 loss_train: 0.3647 acc_train: 0.8867 loss_val: 0.5017 acc_val: 0.8838 time: 0.0097s\n",
            "Epoch: 095 loss_train: 0.3586 acc_train: 0.8947 loss_val: 0.5037 acc_val: 0.8801 time: 0.0089s\n",
            "Epoch: 096 loss_train: 0.3616 acc_train: 0.8781 loss_val: 0.5067 acc_val: 0.8801 time: 0.0089s\n",
            "Epoch: 097 loss_train: 0.3585 acc_train: 0.8842 loss_val: 0.5094 acc_val: 0.8801 time: 0.0099s\n",
            "Epoch: 098 loss_train: 0.3497 acc_train: 0.8922 loss_val: 0.5146 acc_val: 0.8764 time: 0.0092s\n",
            "Epoch: 099 loss_train: 0.3401 acc_train: 0.9015 loss_val: 0.5130 acc_val: 0.8782 time: 0.0088s\n",
            "Epoch: 100 loss_train: 0.3308 acc_train: 0.8898 loss_val: 0.5083 acc_val: 0.8801 time: 0.0088s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 1.8522s\n",
            "\n",
            "Test set results: loss= 0.6522 accuracy= 0.8358\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_t_sne.py:1162: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
            "  warnings.warn(\n",
            "Figure(1500x900)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gLoVARPYdzxx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}