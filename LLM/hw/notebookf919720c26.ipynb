{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80517dbc",
   "metadata": {
    "id": "80517dbc"
   },
   "source": [
    "# Teach an LLM to do additions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaca18f",
   "metadata": {
    "id": "0aaca18f"
   },
   "source": [
    "The goal of this project is to teach an LLM to do additions, playing only with two parts:\n",
    "* the tokenizer\n",
    "* the positional embedding\n",
    "\n",
    "Both the model and the dataset are fixed.\n",
    "\n",
    "You are allowed to tune the hyperparameters, but this is not the main goal. Depending on the quality of your tokenizer and positional embedding, you may change the number of bits. The initial value of 3 is very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ae993bb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T17:22:16.264095Z",
     "iopub.status.busy": "2025-02-05T17:22:16.263812Z",
     "iopub.status.idle": "2025-02-05T17:22:16.268599Z",
     "shell.execute_reply": "2025-02-05T17:22:16.267390Z",
     "shell.execute_reply.started": "2025-02-05T17:22:16.264073Z"
    },
    "id": "ae993bb9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "OzGh9ahKF17h",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T17:22:17.181590Z",
     "iopub.status.busy": "2025-02-05T17:22:17.181227Z",
     "iopub.status.idle": "2025-02-05T17:22:17.185587Z",
     "shell.execute_reply": "2025-02-05T17:22:17.184624Z",
     "shell.execute_reply.started": "2025-02-05T17:22:17.181560Z"
    },
    "id": "OzGh9ahKF17h",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "number_bits = 3\n",
    "\n",
    "dataset_size = 64_000\n",
    "train_proportion = 0.9\n",
    "\n",
    "log_interval = 200\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "learning_rate = 8e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c054bed",
   "metadata": {
    "id": "6c054bed"
   },
   "source": [
    "## Step 1: Construct a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "t6aC9uNeIR6C",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T17:22:17.770758Z",
     "iopub.status.busy": "2025-02-05T17:22:17.770380Z",
     "iopub.status.idle": "2025-02-05T17:22:17.774488Z",
     "shell.execute_reply": "2025-02-05T17:22:17.773521Z",
     "shell.execute_reply.started": "2025-02-05T17:22:17.770727Z"
    },
    "id": "t6aC9uNeIR6C",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pad_token=\"[PAD]\"\n",
    "eos_token=\"[EOS]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BMvT0B-MGBnY",
   "metadata": {
    "id": "BMvT0B-MGBnY"
   },
   "source": [
    "### Baseline: character-level tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "g2QiF-otFur3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T17:51:58.444312Z",
     "iopub.status.busy": "2025-02-05T17:51:58.443972Z",
     "iopub.status.idle": "2025-02-05T17:51:58.453081Z",
     "shell.execute_reply": "2025-02-05T17:51:58.452051Z",
     "shell.execute_reply.started": "2025-02-05T17:51:58.444280Z"
    },
    "id": "g2QiF-otFur3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class character_level_tokenizer:\n",
    "    \"\"\"\n",
    "    character-level\n",
    "    \"\"\"\n",
    "    def __init__(self, reverse=True):\n",
    "        self.reverse = reverse\n",
    "        pad_token=\"[PAD]\"\n",
    "        eos_token=\"[EOS]\"\n",
    "        self.vocab = [str(x) for x in range(10)] + [\"+\", \"=\"] + [pad_token, eos_token]\n",
    "        self.token_to_id = {v : k for k, v in enumerate(self.vocab)}\n",
    "        self.id_to_token = {k : v for k, v in enumerate(self.vocab)}\n",
    "        self.ntokens = len(self.vocab)\n",
    "        self.pattern = f\"[^{re.escape(''.join(self.vocab))}]\"\n",
    "\n",
    "    def clean(self, text):\n",
    "        \"\"\"\n",
    "        removes all characters not in the vocabulary\n",
    "        \"\"\"\n",
    "        out = re.sub(self.pattern, \"\", text)\n",
    "        return out\n",
    "\n",
    "    def pre_tokenization(self, text):\n",
    "        \"\"\"\n",
    "        character-level\n",
    "        \"\"\"\n",
    "\n",
    "        if self.reverse:\n",
    "          if '+' in text:\n",
    "            a, b = text.split('+')\n",
    "            c = ''\n",
    "            if '=' in text:\n",
    "              b, c = b.split('=')\n",
    "            #a, b, c = a[::-1], b[::-1], c[::-1]\n",
    "            a, b, c = a[::-1], b[::-1], c\n",
    "            ret = ''\n",
    "            for i in range(max(len(a), len(b))):\n",
    "                if i < len(a):\n",
    "                   ret += a[i]\n",
    "                else: ret += '0'\n",
    "                ret += '+'\n",
    "                if i < len(b):\n",
    "                   ret += b[i]\n",
    "                else: ret += '0' \n",
    "                ret += '='\n",
    "            text = ret[:-1] + '=' + c\n",
    "          else:\n",
    "            #text = text[::-1]\n",
    "              pass\n",
    "        return [c for c in text]\n",
    "\n",
    "    def encode(self, text):\n",
    "        text_list = self.pre_tokenization(self.clean(text))\n",
    "        return [self.token_to_id[c] for c in text_list]\n",
    "\n",
    "    def decode(self, token_list):\n",
    "        reverse =  \"\".join([self.id_to_token[x] for x in token_list])\n",
    "        reverse = reverse.replace('[PAD]', '')\n",
    "        reverse = reverse.replace('[EOS]', '')\n",
    "        return reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "QuCc6jF5F8hK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-05T17:52:00.794365Z",
     "iopub.status.busy": "2025-02-05T17:52:00.794067Z",
     "iopub.status.idle": "2025-02-05T17:52:00.799486Z",
     "shell.execute_reply": "2025-02-05T17:52:00.798628Z",
     "shell.execute_reply.started": "2025-02-05T17:52:00.794340Z"
    },
    "id": "QuCc6jF5F8hK",
    "outputId": "5985e987-7c95-48a2-e06c-007170553130",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = character_level_tokenizer()\n",
    "ntokens = tokenizer.ntokens\n",
    "ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "3a069151-7bfa-4d80-8705-35845fabe076",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T17:57:46.278684Z",
     "iopub.status.busy": "2025-02-05T17:57:46.278368Z",
     "iopub.status.idle": "2025-02-05T17:57:46.288480Z",
     "shell.execute_reply": "2025-02-05T17:57:46.287589Z",
     "shell.execute_reply.started": "2025-02-05T17:57:46.278661Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [3, 11, 9, 12, 0, 11, 9, 12, 7, 11, 4, 12]\n",
      "Decoded: 3+9=0+9=7+4=\n"
     ]
    }
   ],
   "source": [
    "class ColumnWiseAdditionTokenizer:\n",
    "    def __init__(self):\n",
    "        self.vocab = [str(i) for i in range(11)] + [\"+\", \"=\", pad_token, eos_token]\n",
    "        self.token_to_id = {v: k for k, v in enumerate(self.vocab)}\n",
    "        self.id_to_token = {k: v for v, k in self.token_to_id.items()}\n",
    "        self.ntokens = len(self.vocab)\n",
    "        self.pattern = f\"[^{re.escape(''.join(self.vocab))}]\"\n",
    "\n",
    "    def clean(self, text):\n",
    "        \"\"\"\n",
    "        Removes all characters not in the vocabulary\n",
    "        \"\"\"\n",
    "        return re.sub(self.pattern, \"\", text)\n",
    "\n",
    "    def pre_tokenization(self, num1, num2=None):\n",
    "        \"\"\"\n",
    "        Returns a list of characters.\n",
    "        Tokenizes an addition into a column-wise addition sequence\n",
    "        If only num1 is provided, it returns a simple tokenized number.\n",
    "        \"\"\"\n",
    "        num1 = str(num1)\n",
    "        if num2 is None:\n",
    "            return list(num1)\n",
    "        \n",
    "        num2 = str(num2)\n",
    "        max_len = max(len(num1), len(num2))\n",
    "        num1, num2 = num1.zfill(max_len), num2.zfill(max_len)  # Padding with zeros\n",
    "        \n",
    "        tokens = []\n",
    "        for i in range(max_len - 1, -1, -1):  # Start from least significant digit\n",
    "            tokens.append(num1[i])\n",
    "            tokens.append(\"+\")\n",
    "            tokens.append(num2[i])\n",
    "            tokens.append(\"=\")\n",
    "        return tokens\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Extract num1 and num2 from the string input and tokenize accordingly\n",
    "        \"\"\"\n",
    "        match = re.match(r\"(\\d+)\\+(\\d+)=\", text)\n",
    "        if match:\n",
    "            num1, num2 = match.groups()\n",
    "            token_list = self.pre_tokenization(num1, num2)\n",
    "        else:\n",
    "            # If the input is just a number, encode it as digits\n",
    "            token_list = self.pre_tokenization(text)\n",
    "        \n",
    "        return [self.token_to_id[t] for t in token_list if t in self.token_to_id]\n",
    "    \n",
    "    def decode(self, token_list):\n",
    "        return \"\".join([self.id_to_token[x] for x in token_list if x in self.id_to_token])\n",
    "\n",
    "# Exemple d'utilisation\n",
    "tokenizer = ColumnWiseAdditionTokenizer()\n",
    "ntokens = tokenizer.ntokens\n",
    "encoded = tokenizer.encode('703+499=')\n",
    "print(\"Encoded:\", encoded)\n",
    "print(\"Decoded:\", tokenizer.decode(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "8FXW2K-1Jd-P",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-05T17:57:47.926480Z",
     "iopub.status.busy": "2025-02-05T17:57:47.926163Z",
     "iopub.status.idle": "2025-02-05T17:57:47.932555Z",
     "shell.execute_reply": "2025-02-05T17:57:47.931576Z",
     "shell.execute_reply.started": "2025-02-05T17:57:47.926452Z"
    },
    "id": "8FXW2K-1Jd-P",
    "outputId": "8bbcfcc3-fa30-4dac-f77b-38cd0d61c996",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1, 2], '12')"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"12\"\n",
    "inputs = tokenizer.encode(prompt)\n",
    "print(inputs)\n",
    "inputs, tokenizer.decode(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "52bd0e5d-bba8-4f80-bbcc-aff8082b2582",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T17:57:51.189170Z",
     "iopub.status.busy": "2025-02-05T17:57:51.188892Z",
     "iopub.status.idle": "2025-02-05T17:57:51.195301Z",
     "shell.execute_reply": "2025-02-05T17:57:51.194394Z",
     "shell.execute_reply.started": "2025-02-05T17:57:51.189148Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 11, 4, 1, 12, 1, 6, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1, 2, 2, 11, 4, 1, 12, 1, 6, 3], '122+41=163')"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"122 + 41 = 163\"\n",
    "inputs = tokenizer.encode(prompt)\n",
    "print(inputs)\n",
    "inputs, tokenizer.decode(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j3gckvebGGYt",
   "metadata": {
    "id": "j3gckvebGGYt"
   },
   "source": [
    "# Implement your tokenizer here!\n",
    "\n",
    "You can do anything (as long as you do not compute the addition!).\n",
    "Some ideas:\n",
    "* reversing numbers left to right\n",
    "* arranging by groups (of, 2, 3,...)\n",
    "* aligning numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491af297",
   "metadata": {
    "id": "491af297"
   },
   "source": [
    "## Step 2: Create a dataset for arithmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "daa90f31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-05T17:57:51.666454Z",
     "iopub.status.busy": "2025-02-05T17:57:51.666151Z",
     "iopub.status.idle": "2025-02-05T17:57:51.673156Z",
     "shell.execute_reply": "2025-02-05T17:57:51.672138Z",
     "shell.execute_reply.started": "2025-02-05T17:57:51.666428Z"
    },
    "id": "daa90f31",
    "outputId": "f9582c50-2e76-48b1-a7c2-09df78118e2d",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('151+834=', '985')"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample_datapoint(number_bits = 3):\n",
    "    \"\"\"\n",
    "    returns a string containing two random numbers on `number_bits` many bits and their sum.\n",
    "    \"\"\"\n",
    "    a_list = [random.randint(0, 9) for _ in range(number_bits)]\n",
    "    b_list = [random.randint(0, 9) for _ in range(number_bits)]\n",
    "    a_int = int(\"\".join([str(x) for x in a_list]))\n",
    "    b_int = int(\"\".join([str(x) for x in b_list]))\n",
    "    sum_int = a_int + b_int\n",
    "    return (str(a_int) + \"+\" + str(b_int) + \"=\", str(sum_int))\n",
    "\n",
    "sample_datapoint(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "b6e861d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-05T17:57:51.894011Z",
     "iopub.status.busy": "2025-02-05T17:57:51.893735Z",
     "iopub.status.idle": "2025-02-05T17:57:52.387489Z",
     "shell.execute_reply": "2025-02-05T17:57:52.386788Z",
     "shell.execute_reply.started": "2025-02-05T17:57:51.893988Z"
    },
    "id": "b6e861d2",
    "outputId": "40bbaa66-8aeb-419a-9dc1-baf997af101d",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('682+275=', '957'),\n",
       " ('952+575=', '1527'),\n",
       " ('365+595=', '960'),\n",
       " ('907+918=', '1825')]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for _ in range(dataset_size):\n",
    "    data.append(sample_datapoint(number_bits))\n",
    "data[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "fee85050",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-05T17:57:52.388713Z",
     "iopub.status.busy": "2025-02-05T17:57:52.388453Z",
     "iopub.status.idle": "2025-02-05T17:57:52.397918Z",
     "shell.execute_reply": "2025-02-05T17:57:52.396995Z",
     "shell.execute_reply.started": "2025-02-05T17:57:52.388679Z"
    },
    "id": "fee85050",
    "outputId": "98a5a5e5-0381-4f90-8df0-d78c62f22350",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57600, 6400)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = data[: int(train_proportion * dataset_size)]\n",
    "data_test = data[int(train_proportion * dataset_size):]\n",
    "\n",
    "len(data_train),len(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37200598",
   "metadata": {
    "id": "37200598"
   },
   "source": [
    "## Step 3: Construct a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd7d2eb",
   "metadata": {
    "id": "0fd7d2eb"
   },
   "source": [
    "### Basline: the classical Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91674239",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T17:57:53.977409Z",
     "iopub.status.busy": "2025-02-05T17:57:53.977135Z",
     "iopub.status.idle": "2025-02-05T17:57:53.984501Z",
     "shell.execute_reply": "2025-02-05T17:57:53.983624Z",
     "shell.execute_reply.started": "2025-02-05T17:57:53.977389Z"
    },
    "id": "91674239",
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPositionalEmbedding\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m        Here, we use sine and cosine functions of different frequencies.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m        max_len: the max. length of the incoming sequence (default=5000).\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, d_model, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
    "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
    "        Here, we use sine and cosine functions of different frequencies.\n",
    "    .. math:\n",
    "        \\text{PosEmbedder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEmbedder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "        self.scale = nn.Parameter(torch.ones(d_model))\n",
    "        self.position = nn.Embedding(max_len, d_model)\n",
    "\n",
    "    def forward(self, x, log_scale=False):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        \"\"\"\n",
    "\n",
    "        if log_scale:\n",
    "            positions = torch.arange(x.size(0), device=x.device).unsqueeze(1)\n",
    "            pos_embed = self.position(positions)\n",
    "            log_scale = torch.log10(positions.float() + 1).unsqueeze(-1)\n",
    "            return x * self.scale + pos_embed * log_scale\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "\n",
    "\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8296ceb2",
   "metadata": {
    "id": "8296ceb2"
   },
   "source": [
    "# Implement your positional embedding here!\n",
    "\n",
    "You can do anything. Some ideas:\n",
    "* RoPE\n",
    "* (randomised) FIRE\n",
    "* Abacus\n",
    "\n",
    "**!!! IMPORTANT !!!** This model of Transformers is \"input first\", meaning that an input is a tensor with shape\n",
    "(length_prompts, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "4eb278ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T17:57:54.254856Z",
     "iopub.status.busy": "2025-02-05T17:57:54.254611Z",
     "iopub.status.idle": "2025-02-05T17:57:54.261166Z",
     "shell.execute_reply": "2025-02-05T17:57:54.260150Z",
     "shell.execute_reply.started": "2025-02-05T17:57:54.254837Z"
    },
    "id": "4eb278ab",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Transformer):\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__(d_model=ninp,\n",
    "                                               nhead=nhead,\n",
    "                                               dim_feedforward=nhid,\n",
    "                                               num_encoder_layers=nlayers)\n",
    "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
    "        self.pos_encoder = PositionalEmbedding(ninp, dropout)\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.ninp = ninp\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.bias)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        return torch.log(torch.tril(torch.ones(sz,sz)))\n",
    "\n",
    "    def forward(self, src):\n",
    "        mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "        self.src_mask = mask\n",
    "\n",
    "        src = self.input_emb(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output_enc = self.encoder(src, mask=self.src_mask)\n",
    "        output_dec = self.decoder(output_enc)\n",
    "        return F.log_softmax(output_dec, dim=-1), output_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "42f9d1ee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-05T17:57:54.402668Z",
     "iopub.status.busy": "2025-02-05T17:57:54.402432Z",
     "iopub.status.idle": "2025-02-05T17:57:54.407036Z",
     "shell.execute_reply": "2025-02-05T17:57:54.406266Z",
     "shell.execute_reply.started": "2025-02-05T17:57:54.402648Z"
    },
    "id": "42f9d1ee",
    "outputId": "0c1a66e5-2886-4024-9722-2564b2b82667",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "1d568cc4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-05T17:57:54.516824Z",
     "iopub.status.busy": "2025-02-05T17:57:54.516554Z",
     "iopub.status.idle": "2025-02-05T17:57:54.579310Z",
     "shell.execute_reply": "2025-02-05T17:57:54.578412Z",
     "shell.execute_reply.started": "2025-02-05T17:57:54.516800Z"
    },
    "id": "1d568cc4",
    "outputId": "f3200239-cc4e-4a29-b5a0-c90c7ced4c2f",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Linear(in_features=128, out_features=15, bias=True)\n",
       "  (input_emb): Embedding(15, 128)\n",
       "  (pos_encoder): PositionalEmbedding(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (position): Embedding(5000, 128)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerModel(ntoken = ntokens,\n",
    "                         ninp = 128,\n",
    "                         nhead = 16,\n",
    "                         nhid = 64,\n",
    "                         nlayers = 8)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "8f2f06e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T17:57:56.003229Z",
     "iopub.status.busy": "2025-02-05T17:57:56.002913Z",
     "iopub.status.idle": "2025-02-05T17:57:56.007805Z",
     "shell.execute_reply": "2025-02-05T17:57:56.006952Z",
     "shell.execute_reply.started": "2025-02-05T17:57:56.003201Z"
    },
    "id": "8f2f06e0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate(model, prompts, new_tokens = 5):\n",
    "    input_tensor = prompts # (length_prompts, batch_size)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    for _ in range(new_tokens):\n",
    "        output, _ = model(input_tensor) # (length_prompts, batch_size, ntokens)\n",
    "        last_output = output[-1,:,:] # (batch_size, ntokens)\n",
    "        token = torch.argmax(last_output, -1).view((1,-1)) # (1, batch_size)\n",
    "        input_tensor = torch.cat((input_tensor, token), 0)\n",
    "    return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "d76d1b19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-05T17:57:56.244701Z",
     "iopub.status.busy": "2025-02-05T17:57:56.244433Z",
     "iopub.status.idle": "2025-02-05T17:57:56.282969Z",
     "shell.execute_reply": "2025-02-05T17:57:56.282182Z",
     "shell.execute_reply.started": "2025-02-05T17:57:56.244679Z"
    },
    "id": "d76d1b19",
    "outputId": "6b217095-2364-4b92-c20e-b98d149f9b36",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2, 11,  3, 12, 13, 13, 13, 13, 13]], device='cuda:0'),\n",
       " '2+3=[PAD][PAD][PAD][PAD][PAD]')"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "prompt = \"2+3=\"\n",
    "prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
    "output = generate(model, prompt_tensor).view((1,-1))\n",
    "output, tokenizer.decode(output.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "bc5cc28e-0ba9-4a71-abcc-7ce5fcbb665b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T17:57:56.343656Z",
     "iopub.status.busy": "2025-02-05T17:57:56.343424Z",
     "iopub.status.idle": "2025-02-05T17:57:56.348393Z",
     "shell.execute_reply": "2025-02-05T17:57:56.347587Z",
     "shell.execute_reply.started": "2025-02-05T17:57:56.343635Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 11, 3, 12]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "43f43aa6-f324-4c1c-839b-336309504727",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T17:57:56.472665Z",
     "iopub.status.busy": "2025-02-05T17:57:56.472423Z",
     "iopub.status.idle": "2025-02-05T17:57:56.510911Z",
     "shell.execute_reply": "2025-02-05T17:57:56.510010Z",
     "shell.execute_reply.started": "2025-02-05T17:57:56.472644Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2, 11,  3, 12, 13, 13, 13, 13, 13]], device='cuda:0')"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model, prompt_tensor).view((1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "00954ddc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T17:57:56.589352Z",
     "iopub.status.busy": "2025-02-05T17:57:56.589125Z",
     "iopub.status.idle": "2025-02-05T17:57:56.594258Z",
     "shell.execute_reply": "2025-02-05T17:57:56.593227Z",
     "shell.execute_reply.started": "2025-02-05T17:57:56.589332Z"
    },
    "id": "00954ddc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def pad(token_list, type_list = \"prompts\"):\n",
    "    max_length = max([len(x) for x in token_list])\n",
    "    out = []\n",
    "    for x in token_list:\n",
    "        if type_list == \"prompts\":\n",
    "            out.append([tokenizer.token_to_id[pad_token]] * (max_length - len(x)) + x)\n",
    "        if type_list == \"answers\":\n",
    "            out.append(x + [tokenizer.token_to_id[eos_token]] + [tokenizer.token_to_id[pad_token]] * (max_length - len(x)))\n",
    "    return out, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "2c84beab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-05T17:57:56.695897Z",
     "iopub.status.busy": "2025-02-05T17:57:56.695649Z",
     "iopub.status.idle": "2025-02-05T17:57:56.701797Z",
     "shell.execute_reply": "2025-02-05T17:57:56.701033Z",
     "shell.execute_reply.started": "2025-02-05T17:57:56.695877Z"
    },
    "id": "2c84beab",
    "outputId": "f6131c8f-22c9-41f3-a744-347ceb56b412",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[PAD][PAD][PAD][PAD]1+1=', '1+5=2+3='], ['2[EOS][PAD]', '56[EOS]'])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [tokenizer.encode(\"1+1=\"), tokenizer.encode(\"21+35=\")]\n",
    "answers = [tokenizer.encode(\"2\"), tokenizer.encode(\"56\")]\n",
    "padded_prompts, _ = pad(prompts, \"prompts\")\n",
    "padded_answers, _ = pad(answers, \"answers\")\n",
    "padded_prompts, padded_answers\n",
    "[tokenizer.decode(p) for p in padded_prompts], [tokenizer.decode(p) for p in padded_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "c27cc573-e5b5-4648-ab62-c2149f428f3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T17:57:56.835692Z",
     "iopub.status.busy": "2025-02-05T17:57:56.835434Z",
     "iopub.status.idle": "2025-02-05T17:57:56.840499Z",
     "shell.execute_reply": "2025-02-05T17:57:56.839614Z",
     "shell.execute_reply.started": "2025-02-05T17:57:56.835670Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[13, 13, 13, 13, 1, 11, 1, 12], [1, 11, 5, 12, 2, 11, 3, 12]]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "264f9227",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T17:57:56.967227Z",
     "iopub.status.busy": "2025-02-05T17:57:56.967014Z",
     "iopub.status.idle": "2025-02-05T17:57:56.971882Z",
     "shell.execute_reply": "2025-02-05T17:57:56.971063Z",
     "shell.execute_reply.started": "2025-02-05T17:57:56.967208Z"
    },
    "id": "264f9227",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_batch(split, i):\n",
    "    data = data_train if split == 'train' else data_test\n",
    "    prompts = [tokenizer.encode(data[i][0]) for i in range(i, i + batch_size)]\n",
    "    padded_prompts, length_prompts = pad(prompts, \"prompts\")\n",
    "    answers = [tokenizer.encode(data[i][1]) for i in range(i, i + batch_size)]\n",
    "    padded_answers, length_answers = pad(answers, \"answers\")\n",
    "    X = torch.stack([torch.tensor(x) for x in padded_prompts], 1)\n",
    "    Y = torch.stack([torch.tensor(x) for x in padded_answers], 1)\n",
    "    return X, Y, length_prompts, length_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "91e281ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-05T17:57:58.737323Z",
     "iopub.status.busy": "2025-02-05T17:57:58.736989Z",
     "iopub.status.idle": "2025-02-05T17:57:58.746066Z",
     "shell.execute_reply": "2025-02-05T17:57:58.745107Z",
     "shell.execute_reply.started": "2025-02-05T17:57:58.737293Z"
    },
    "id": "91e281ad",
    "outputId": "9810ff55-3acb-42a0-e0db-c78b64cf5d21",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 64]), torch.Size([5, 64]), 12, 4)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y, length_prompts, length_answers = get_batch(\"train\", 243)\n",
    "X.shape, Y.shape, length_prompts, length_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "ccc66d65-2faa-4e60-8013-694ce864c425",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T17:57:58.930870Z",
     "iopub.status.busy": "2025-02-05T17:57:58.930623Z",
     "iopub.status.idle": "2025-02-05T17:57:58.939326Z",
     "shell.execute_reply": "2025-02-05T17:57:58.938604Z",
     "shell.execute_reply.started": "2025-02-05T17:57:58.930848Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8,  8,  1,  2,  9,  3,  9,  4,  7,  2,  0,  4,  3,  6,  0,  6,  9,  0,\n",
       "          4,  6,  6,  3,  2,  7,  1,  1,  1,  0,  8,  1,  5,  2,  9,  5,  9,  2,\n",
       "          2,  3,  1,  2,  9,  0,  9,  1,  4,  6,  4,  4,  3,  1, 13,  0,  1,  4,\n",
       "          0,  9,  6,  8,  6,  7,  8,  0,  3,  0],\n",
       "        [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 13, 11, 11, 11,\n",
       "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11],\n",
       "        [ 5,  2,  6,  2,  9,  6,  5,  5,  8,  2,  9,  2,  2,  2,  8,  9,  4,  1,\n",
       "          3,  8,  1,  7,  5,  1,  9,  1,  7,  8,  0,  6,  9,  9,  5,  6,  2,  3,\n",
       "          1,  9,  2,  2,  0,  9,  8,  3,  9,  0,  0,  0,  6,  2, 13,  0,  9,  2,\n",
       "          3,  6,  5,  5,  3,  9,  4,  4,  7,  0],\n",
       "        [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12,\n",
       "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12],\n",
       "        [ 4,  5,  6,  3,  1,  5,  5,  0,  4,  7,  0,  3,  8,  5,  3,  7,  7,  1,\n",
       "          0,  5,  3,  2,  3,  3,  3,  7,  3,  1,  2,  1,  0,  7,  4,  0,  4,  6,\n",
       "          1,  0,  9,  0,  8,  3,  1,  1,  1,  5,  5,  6,  4,  1,  6,  8,  5,  8,\n",
       "          9,  4,  2,  3,  6,  9,  1,  1,  4,  6],\n",
       "        [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11],\n",
       "        [ 3,  1,  8,  5,  5,  0,  6,  9,  3,  1,  4,  1,  2,  8,  4,  6,  0,  6,\n",
       "          9,  1,  3,  6,  6,  9,  1,  6,  8,  2,  8,  8,  2,  5,  8,  3,  3,  5,\n",
       "          7,  2,  0,  6,  3,  9,  3,  7,  7,  9,  4,  0,  1,  1,  6,  9,  8,  1,\n",
       "          3,  5,  5,  2,  7,  8,  7,  0,  1,  5],\n",
       "        [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12],\n",
       "        [ 9,  1,  1,  1,  6,  1,  4,  1,  9,  7,  4,  0,  8,  7,  8,  2,  7,  1,\n",
       "          4,  1,  0,  0,  5,  0,  2,  7,  9,  7,  5,  7,  2,  5,  7,  6,  2,  1,\n",
       "          9,  0,  9,  1,  8,  9,  6,  2,  0,  5,  1,  6,  3,  2,  2,  5,  9,  7,\n",
       "          3,  7,  8,  2,  3,  7,  7,  1,  8,  2],\n",
       "        [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11],\n",
       "        [ 5,  8,  7,  0,  4,  3,  0,  1,  6,  5,  9,  9,  3,  8,  0,  5,  5,  1,\n",
       "          6,  1,  6,  9,  5,  1,  7,  5,  7,  9,  4,  9,  2,  1,  0,  3,  4,  7,\n",
       "          1,  4,  2,  7,  9,  9,  4,  7,  9,  4,  1,  7,  1,  0,  1,  6,  4,  7,\n",
       "          0,  6,  5,  7,  2,  4,  7,  5,  6,  5],\n",
       "        [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12]])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "a4ee0ee3-ccd1-429c-ad0c-2d7ca9a8e13b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T17:57:59.030541Z",
     "iopub.status.busy": "2025-02-05T17:57:59.030337Z",
     "iopub.status.idle": "2025-02-05T17:57:59.036689Z",
     "shell.execute_reply": "2025-02-05T17:57:59.035871Z",
     "shell.execute_reply.started": "2025-02-05T17:57:59.030524Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  9,  9,  1,  1,  4,  5,  2,  1,  1,  1,  9,  1,  1,  8,  8,  1,  2,\n",
       "          1,  2,  6,  9,  1,  2,  9,  1,  1,  1,  1,  1,  4,  7,  8,  9,  6,  9,\n",
       "          1,  4,  1,  8,  1,  1,  1,  9,  9,  1,  2,  1,  4,  2,  4,  1,  1,  1,\n",
       "          4,  1,  1,  9,  6,  1,  1,  6,  1,  8],\n",
       "        [ 4,  7,  4,  8,  0,  5,  2,  9,  5,  2,  3,  4,  2,  6,  7,  4,  2,  7,\n",
       "          0,  7,  6,  9,  0,  2,  5,  3,  7,  6,  0,  6,  3,  3,  3,  4,  8,  1,\n",
       "          0,  3,  1,  6,  8,  9,  0,  8,  9,  0,  9,  3,  5,  2,  2,  2,  4,  4,\n",
       "          2,  4,  3,  6,  3,  2,  4,  1,  4,  1],\n",
       "        [ 8,  0,  7,  4,  7,  9,  4,  9,  8,  8,  4,  6,  0,  3,  8,  5,  8,  1,\n",
       "          9,  4,  7,  0,  9,  8,  0,  3,  1,  3,  0,  9,  4,  1,  4,  1,  1,  5,\n",
       "          8,  2,  9,  4,  1,  2,  5,  4,  3,  4,  4,  6,  9,  3, 14,  7,  4,  9,\n",
       "          3,  0,  8,  3,  9,  8,  9,  4,  6,  0],\n",
       "        [ 3, 14, 14, 14,  8, 14, 14, 14,  5,  4,  9, 14,  5,  8, 14, 14,  3, 14,\n",
       "          7, 14, 14, 14,  7, 14, 14,  2,  8,  8,  8,  7, 14, 14, 14, 14, 14, 14,\n",
       "          3, 14,  3, 14,  9,  9,  7, 14, 14,  6, 14,  4, 14, 14, 13,  0,  0,  6,\n",
       "         14,  5,  1, 14, 14,  6,  2, 14,  0, 14],\n",
       "        [14, 13, 13, 13, 14, 13, 13, 13, 14, 14, 14, 13, 14, 14, 13, 13, 14, 13,\n",
       "         14, 13, 13, 13, 14, 13, 13, 14, 14, 14, 14, 14, 13, 13, 13, 13, 13, 13,\n",
       "         14, 13, 14, 13, 14, 14, 14, 13, 13, 14, 13, 14, 13, 13, 13, 14, 14, 14,\n",
       "         13, 14, 14, 13, 13, 14, 14, 13, 14, 13]])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113e1fd1",
   "metadata": {
    "id": "113e1fd1"
   },
   "source": [
    "## Step 4: Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "1cfcd10a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T17:58:01.363247Z",
     "iopub.status.busy": "2025-02-05T17:58:01.362980Z",
     "iopub.status.idle": "2025-02-05T17:58:01.368622Z",
     "shell.execute_reply": "2025-02-05T17:58:01.367712Z",
     "shell.execute_reply.started": "2025-02-05T17:58:01.363226Z"
    },
    "id": "1cfcd10a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    # Turn on evaluation mode disables dropout.\n",
    "    model.eval()\n",
    "    correct = 0.\n",
    "    with torch.no_grad():\n",
    "        for batch, i in enumerate(range(0, len(data_test) - 1, batch_size)):\n",
    "            prompts, target_answers, length_prompts, length_answers = get_batch(\"test\", i)\n",
    "            prompts = prompts.to(device) # (length_prompts, batch_size)\n",
    "            target_answers = target_answers.to(device) # (length_answers + 1, batch_size)\n",
    "            output = generate(model, prompts, length_answers + 1) # (length_prompts + length_answers + 1, batch_size)\n",
    "            answers_tokens = output[length_prompts:, :] # (length_answers + 1, batch_size), contains tokens\n",
    "            equality_test = answers_tokens == target_answers # (length_answers + 1, batch_size), contains boolean values\n",
    "            correct += torch.all(equality_test, axis=0).float().sum()\n",
    "        accuracy = correct / len(data_test)\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "ac335b05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-05T17:58:03.354403Z",
     "iopub.status.busy": "2025-02-05T17:58:03.354134Z",
     "iopub.status.idle": "2025-02-05T17:58:05.884904Z",
     "shell.execute_reply": "2025-02-05T17:58:05.883978Z",
     "shell.execute_reply.started": "2025-02-05T17:58:03.354383Z"
    },
    "id": "ac335b05",
    "outputId": "4be36aea-9aa6-4b7e-9d6e-9fa82d5cdc92",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c54061a",
   "metadata": {
    "id": "4c54061a"
   },
   "source": [
    "## Step 4: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "3638a75d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T17:58:05.886707Z",
     "iopub.status.busy": "2025-02-05T17:58:05.886336Z",
     "iopub.status.idle": "2025-02-05T17:58:05.894807Z",
     "shell.execute_reply": "2025-02-05T17:58:05.893828Z",
     "shell.execute_reply.started": "2025-02-05T17:58:05.886671Z"
    },
    "id": "3638a75d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_epoch():\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    for batch, i in enumerate(range(0, len(data_train) - 1, batch_size)):\n",
    "        prompts, target_answers, length_prompts, length_answers = get_batch(\"train\", i)\n",
    "        prompts = prompts.to(device) # (length_prompts, batch_size)\n",
    "        target_answers = target_answers.to(device) # (length_answers, batch_size)\n",
    "        input_tensor = torch.cat((prompts, target_answers), 0) # (length_prompts + length_answers, batch_size)\n",
    "        model.zero_grad()\n",
    "        output, _ = model(input_tensor) # (length_prompts + length_answers, batch_size, ntokens)\n",
    "        output_answers = output[length_prompts-1:-1,:,:].reshape(-1, ntokens) # (length_answers * batch_size, ntokens)\n",
    "        target_answers = target_answers.view(-1)\n",
    "        loss = F.cross_entropy(output_answers, target_answers)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.2f} | perplexity {:8.2f}'.format(batch, len(data_train) // batch_size,\n",
    "                                                                                                        elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def train():\n",
    "    best_test_accuracy = None\n",
    "    test_accuracy = evaluate()\n",
    "    print('-' * 89)\n",
    "    print('| initialisation | test accuracy {:5.2f}'.format(test_accuracy))\n",
    "    print('-' * 89)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train_epoch()\n",
    "        test_accuracy = evaluate()\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:5.2f}'.format(epoch, (time.time() - epoch_start_time), test_accuracy))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the test accuracy is the best we've seen so far.\n",
    "        if not best_test_accuracy or test_accuracy < best_test_accuracy:\n",
    "            with open(\"arithmetic.pt\", 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_test_accuracy = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "4e2a8490",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-05T17:58:10.996061Z",
     "iopub.status.busy": "2025-02-05T17:58:10.995766Z",
     "iopub.status.idle": "2025-02-05T18:01:38.035867Z",
     "shell.execute_reply": "2025-02-05T18:01:38.034870Z",
     "shell.execute_reply.started": "2025-02-05T17:58:10.996035Z"
    },
    "id": "4e2a8490",
    "outputId": "32b1d592-612e-4c42-fbd7-c4ff236a0e0b",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| initialisation | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 20.87 | loss  1.82 | perplexity     6.19\n",
      "|   400/  900 batches | ms/batch 20.46 | loss  1.42 | perplexity     4.16\n",
      "|   600/  900 batches | ms/batch 19.33 | loss  1.29 | perplexity     3.64\n",
      "|   800/  900 batches | ms/batch 19.54 | loss  1.21 | perplexity     3.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 20.55s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 19.68 | loss  1.17 | perplexity     3.23\n",
      "|   400/  900 batches | ms/batch 19.85 | loss  1.12 | perplexity     3.05\n",
      "|   600/  900 batches | ms/batch 19.90 | loss  1.10 | perplexity     3.01\n",
      "|   800/  900 batches | ms/batch 19.73 | loss  1.08 | perplexity     2.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 20.30s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 19.71 | loss  1.06 | perplexity     2.90\n",
      "|   400/  900 batches | ms/batch 19.58 | loss  1.04 | perplexity     2.82\n",
      "|   600/  900 batches | ms/batch 19.73 | loss  1.03 | perplexity     2.81\n",
      "|   800/  900 batches | ms/batch 19.59 | loss  1.01 | perplexity     2.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 20.10s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 20.56 | loss  1.00 | perplexity     2.71\n",
      "|   400/  900 batches | ms/batch 19.90 | loss  0.92 | perplexity     2.52\n",
      "|   600/  900 batches | ms/batch 19.17 | loss  0.88 | perplexity     2.40\n",
      "|   800/  900 batches | ms/batch 19.78 | loss  0.79 | perplexity     2.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 20.30s | test accuracy  0.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 20.27 | loss  0.71 | perplexity     2.04\n",
      "|   400/  900 batches | ms/batch 20.00 | loss  0.68 | perplexity     1.98\n",
      "|   600/  900 batches | ms/batch 19.73 | loss  0.67 | perplexity     1.95\n",
      "|   800/  900 batches | ms/batch 21.14 | loss  0.64 | perplexity     1.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 20.69s | test accuracy  0.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 19.92 | loss  0.63 | perplexity     1.89\n",
      "|   400/  900 batches | ms/batch 19.66 | loss  0.62 | perplexity     1.85\n",
      "|   600/  900 batches | ms/batch 19.72 | loss  0.62 | perplexity     1.85\n",
      "|   800/  900 batches | ms/batch 20.21 | loss  0.62 | perplexity     1.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 20.37s | test accuracy  0.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 19.55 | loss  0.61 | perplexity     1.84\n",
      "|   400/  900 batches | ms/batch 21.20 | loss  0.59 | perplexity     1.80\n",
      "|   600/  900 batches | ms/batch 20.14 | loss  0.58 | perplexity     1.79\n",
      "|   800/  900 batches | ms/batch 19.92 | loss  0.58 | perplexity     1.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 20.60s | test accuracy  0.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 19.75 | loss  0.57 | perplexity     1.77\n",
      "|   400/  900 batches | ms/batch 19.69 | loss  0.57 | perplexity     1.77\n",
      "|   600/  900 batches | ms/batch 20.43 | loss  0.60 | perplexity     1.81\n",
      "|   800/  900 batches | ms/batch 21.31 | loss  0.58 | perplexity     1.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 20.72s | test accuracy  0.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 19.79 | loss  0.57 | perplexity     1.76\n",
      "|   400/  900 batches | ms/batch 19.28 | loss  0.57 | perplexity     1.76\n",
      "|   600/  900 batches | ms/batch 19.56 | loss  0.56 | perplexity     1.76\n",
      "|   800/  900 batches | ms/batch 19.72 | loss  0.56 | perplexity     1.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 20.18s | test accuracy  0.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 20.08 | loss  0.55 | perplexity     1.74\n",
      "|   400/  900 batches | ms/batch 21.35 | loss  0.55 | perplexity     1.73\n",
      "|   600/  900 batches | ms/batch 19.53 | loss  0.55 | perplexity     1.74\n",
      "|   800/  900 batches | ms/batch 19.82 | loss  0.55 | perplexity     1.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 20.65s | test accuracy  0.11\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "56d9d440",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-05T17:42:28.784103Z",
     "iopub.status.busy": "2025-02-05T17:42:28.783749Z",
     "iopub.status.idle": "2025-02-05T17:42:29.226529Z",
     "shell.execute_reply": "2025-02-05T17:42:29.225472Z",
     "shell.execute_reply.started": "2025-02-05T17:42:28.784073Z"
    },
    "id": "56d9d440",
    "outputId": "6892df81-7481-4a8a-a6c3-a3aa45bc3a7f",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9+2=1+8=4+8=1301\t actual result: 1301\n",
      "5+6=0+8=6+4=1091\t actual result: 1091\n",
      "6+4=5+2=3+9=1280\t actual result: 1280\n",
      "6+7=1+5=2+1=373\t actual result: 373\n",
      "1+9=1+6=3+9=1280\t actual result: 1280\n",
      "3+4=8+9=2+8=1177\t actual result: 1177\n",
      "5+9=2+5=2+0=284\t actual result: 284\n",
      "9+8=8+7=9+6=1667\t actual result: 1667\n",
      "8+4=4+8=2+0=332\t actual result: 332\n",
      "7+4=4+5=8+2=1101\t actual result: 1101\n",
      "1+9=0+3=1+2=340\t actual result: 340\n",
      "4+0=5+1=5+3=864\t actual result: 864\n",
      "4+4=0+9=9+3=1298\t actual result: 1298\n",
      "9+8=7+8=0+1=267\t actual result: 267\n",
      "3+1=2+7=7+4=1194\t actual result: 1194\n",
      "8+8=9+6=9+9=1966\t actual result: 1966\n",
      "7+6=1+5=2+2=473\t actual result: 473\n",
      "9+7=5+1=7+6=1376\t actual result: 1376\n",
      "1+0=1+1=9+1=1021\t actual result: 1021\n",
      "7+9=0+7=7+0=786\t actual result: 786\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "for i in range(20):\n",
    "    prompt, answers = data_test[i]\n",
    "    prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
    "    output = generate(model, prompt_tensor, len(answers)).view((1,-1))\n",
    "    print(tokenizer.decode(output.tolist()[0]) + \"\\t actual result: \" + answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qJ9IOZu8Xo4Y",
   "metadata": {
    "id": "qJ9IOZu8Xo4Y"
   },
   "source": [
    "## Probing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78be1213",
   "metadata": {
    "id": "78be1213"
   },
   "source": [
    "This is just for fun..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yomPfirhXkLb",
   "metadata": {
    "id": "yomPfirhXkLb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_size = 1000\n",
    "test_size = 100\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def data_probing(size):\n",
    "    X = []\n",
    "    y = np.zeros(size)\n",
    "    for i in range(size):\n",
    "        input = torch.tensor(tokenizer.encode(data[i][0])).view((-1, 1)).to(device)\n",
    "        _, output = model(input)\n",
    "        output = output[-1,:,:].flatten()\n",
    "        # determine whether there was a carry in the result:\n",
    "        carry = len(data[i][1]) > len(data[i][0]) / 2\n",
    "        X.append(output.cpu().detach().numpy())\n",
    "        y[i] = carry\n",
    "    return np.array(X), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QGmfXVxkppfP",
   "metadata": {
    "id": "QGmfXVxkppfP",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, y_train = data_probing(train_size)\n",
    "X_test, y_test = data_probing(test_size)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "reg = LogisticRegression()\n",
    "reg.fit(X_train,y_train)\n",
    "reg.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
